{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6084669,"sourceType":"datasetVersion","datasetId":3483931},{"sourceId":13063202,"sourceType":"datasetVersion","datasetId":8272516},{"sourceId":13076310,"sourceType":"datasetVersion","datasetId":8281658},{"sourceId":13083134,"sourceType":"datasetVersion","datasetId":8286287},{"sourceId":13296497,"sourceType":"datasetVersion","datasetId":8427397}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install assemblyai pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:33.600376Z","iopub.execute_input":"2025-10-07T08:03:33.600724Z","iopub.status.idle":"2025-10-07T08:03:35.749973Z","shell.execute_reply.started":"2025-10-07T08:03:33.600679Z","shell.execute_reply":"2025-10-07T08:03:35.748384Z"}},"outputs":[{"name":"stdout","text":"The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n^C\n\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# Directory containing the audio files\naudio_directory = '/kaggle/input/911-recordings/911_recordings'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.752303Z","iopub.execute_input":"2025-10-07T08:03:35.752693Z","iopub.status.idle":"2025-10-07T08:03:35.758418Z","shell.execute_reply.started":"2025-10-07T08:03:35.752660Z","shell.execute_reply":"2025-10-07T08:03:35.757329Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# List to store file names and their transcripts\ntranscripts = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.759364Z","iopub.execute_input":"2025-10-07T08:03:35.759659Z","iopub.status.idle":"2025-10-07T08:03:35.764219Z","shell.execute_reply.started":"2025-10-07T08:03:35.759625Z","shell.execute_reply":"2025-10-07T08:03:35.763190Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"# Loop through the specified range\nfor i in range(374, 496):\n    if i in missing_files:\n        continue\n\n    file_name = f\"call_{i}.mp3\"\n    file_path = os.path.join(audio_directory, file_name)\n    \n    # Ensure the file exists\n    if os.path.exists(file_path):\n        try:\n            # Transcribe the audio file\n            transcript_obj = transcriber.transcribe(file_path)\n            transcript_text = transcript_obj.text\n            transcripts.append([file_name, transcript_text])\n        except Exception as e:\n            print(f\"Error transcribing {file_name}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.766802Z","iopub.execute_input":"2025-10-07T08:03:35.767144Z","iopub.status.idle":"2025-10-07T08:03:35.814468Z","shell.execute_reply.started":"2025-10-07T08:03:35.767093Z","shell.execute_reply":"2025-10-07T08:03:35.806401Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3684838650.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loop through the specified range\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m374\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m496\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'missing_files' is not defined"],"ename":"NameError","evalue":"name 'missing_files' is not defined","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"# Create DataFrame\ndf = pd.DataFrame(transcripts, columns=[\"File Name\", \"Transcript\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.815450Z","iopub.status.idle":"2025-10-07T08:03:35.815950Z","shell.execute_reply.started":"2025-10-07T08:03:35.815689Z","shell.execute_reply":"2025-10-07T08:03:35.815705Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.817686Z","iopub.status.idle":"2025-10-07T08:03:35.818001Z","shell.execute_reply.started":"2025-10-07T08:03:35.817869Z","shell.execute_reply":"2025-10-07T08:03:35.817882Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.819170Z","iopub.status.idle":"2025-10-07T08:03:35.819569Z","shell.execute_reply.started":"2025-10-07T08:03:35.819378Z","shell.execute_reply":"2025-10-07T08:03:35.819397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(\"911Section4.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.821538Z","iopub.status.idle":"2025-10-07T08:03:35.821864Z","shell.execute_reply.started":"2025-10-07T08:03:35.821711Z","shell.execute_reply":"2025-10-07T08:03:35.821724Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Section 3: 253 to 373 (excluding 306, 317)","metadata":{}},{"cell_type":"code","source":"# Set up AssemblyAI\naai.settings.api_key = \"4c5a787a36634384b228b0531ebd8c5d\"\ntranscriber = aai.Transcriber()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.823214Z","iopub.status.idle":"2025-10-07T08:03:35.823539Z","shell.execute_reply.started":"2025-10-07T08:03:35.823395Z","shell.execute_reply":"2025-10-07T08:03:35.823408Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of missing files\nmissing_files = {306,317}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.825370Z","iopub.status.idle":"2025-10-07T08:03:35.825945Z","shell.execute_reply.started":"2025-10-07T08:03:35.825648Z","shell.execute_reply":"2025-10-07T08:03:35.825671Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Directory containing the audio files\naudio_directory = '/kaggle/input/911-recordings/911_recordings'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.827348Z","iopub.status.idle":"2025-10-07T08:03:35.827740Z","shell.execute_reply.started":"2025-10-07T08:03:35.827568Z","shell.execute_reply":"2025-10-07T08:03:35.827588Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List to store file names and their transcripts\ntranscripts = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.829101Z","iopub.status.idle":"2025-10-07T08:03:35.829572Z","shell.execute_reply.started":"2025-10-07T08:03:35.829370Z","shell.execute_reply":"2025-10-07T08:03:35.829393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loop through the specified range\nfor i in range(253, 374):\n    if i in missing_files:\n        continue\n\n    file_name = f\"call_{i}.mp3\"\n    file_path = os.path.join(audio_directory, file_name)\n    \n    # Ensure the file exists\n    if os.path.exists(file_path):\n        try:\n            # Transcribe the audio file\n            transcript_obj = transcriber.transcribe(file_path)\n            transcript_text = transcript_obj.text\n            transcripts.append([file_name, transcript_text])\n        except Exception as e:\n            print(f\"Error transcribing {file_name}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.830785Z","iopub.status.idle":"2025-10-07T08:03:35.831186Z","shell.execute_reply.started":"2025-10-07T08:03:35.830987Z","shell.execute_reply":"2025-10-07T08:03:35.831001Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create DataFrame\ndf = pd.DataFrame(transcripts, columns=[\"File Name\", \"Transcript\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.833425Z","iopub.status.idle":"2025-10-07T08:03:35.834717Z","shell.execute_reply.started":"2025-10-07T08:03:35.834463Z","shell.execute_reply":"2025-10-07T08:03:35.834493Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.835915Z","iopub.status.idle":"2025-10-07T08:03:35.836378Z","shell.execute_reply.started":"2025-10-07T08:03:35.836167Z","shell.execute_reply":"2025-10-07T08:03:35.836187Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.837752Z","iopub.status.idle":"2025-10-07T08:03:35.838213Z","shell.execute_reply.started":"2025-10-07T08:03:35.837966Z","shell.execute_reply":"2025-10-07T08:03:35.837984Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(\"911Section3.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.839803Z","iopub.status.idle":"2025-10-07T08:03:35.840148Z","shell.execute_reply.started":"2025-10-07T08:03:35.839968Z","shell.execute_reply":"2025-10-07T08:03:35.839980Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Section 5: 496 to 615 (excluding 448, 449, 495, 500, 504, 509, 511)","metadata":{}},{"cell_type":"code","source":"# Set up AssemblyAI\naai.settings.api_key = \"4c5a787a36634384b228b0531ebd8c5d\"\ntranscriber = aai.Transcriber()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.841376Z","iopub.status.idle":"2025-10-07T08:03:35.841831Z","shell.execute_reply.started":"2025-10-07T08:03:35.841693Z","shell.execute_reply":"2025-10-07T08:03:35.841706Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of missing files\nmissing_files = {448,449,495,500,504,509,511}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.843202Z","iopub.status.idle":"2025-10-07T08:03:35.843583Z","shell.execute_reply.started":"2025-10-07T08:03:35.843374Z","shell.execute_reply":"2025-10-07T08:03:35.843391Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Directory containing the audio files\naudio_directory = '/kaggle/input/911-recordings/911_recordings'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.844672Z","iopub.status.idle":"2025-10-07T08:03:35.845060Z","shell.execute_reply.started":"2025-10-07T08:03:35.844917Z","shell.execute_reply":"2025-10-07T08:03:35.844933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List to store file names and their transcripts\ntranscripts = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.845812Z","iopub.status.idle":"2025-10-07T08:03:35.846070Z","shell.execute_reply.started":"2025-10-07T08:03:35.845943Z","shell.execute_reply":"2025-10-07T08:03:35.845954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loop through the specified range\nfor i in range(496, 616):\n    if i in missing_files:\n        continue\n\n    file_name = f\"call_{i}.mp3\"\n    file_path = os.path.join(audio_directory, file_name)\n    \n    # Ensure the file exists\n    if os.path.exists(file_path):\n        try:\n            # Transcribe the audio file\n            transcript_obj = transcriber.transcribe(file_path)\n            transcript_text = transcript_obj.text\n            transcripts.append([file_name, transcript_text])\n        except Exception as e:\n            print(f\"Error transcribing {file_name}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.846974Z","iopub.status.idle":"2025-10-07T08:03:35.847292Z","shell.execute_reply.started":"2025-10-07T08:03:35.847151Z","shell.execute_reply":"2025-10-07T08:03:35.847169Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create DataFrame\ndf = pd.DataFrame(transcripts, columns=[\"File Name\", \"Transcript\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.849240Z","iopub.status.idle":"2025-10-07T08:03:35.849511Z","shell.execute_reply.started":"2025-10-07T08:03:35.849394Z","shell.execute_reply":"2025-10-07T08:03:35.849406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.850941Z","iopub.status.idle":"2025-10-07T08:03:35.851359Z","shell.execute_reply.started":"2025-10-07T08:03:35.851212Z","shell.execute_reply":"2025-10-07T08:03:35.851229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.852369Z","iopub.status.idle":"2025-10-07T08:03:35.852768Z","shell.execute_reply.started":"2025-10-07T08:03:35.852574Z","shell.execute_reply":"2025-10-07T08:03:35.852597Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(\"911Section5.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.854440Z","iopub.status.idle":"2025-10-07T08:03:35.854750Z","shell.execute_reply.started":"2025-10-07T08:03:35.854607Z","shell.execute_reply":"2025-10-07T08:03:35.854618Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Section 6: 616 to 743 (excluding 630, 636, 639, 727)","metadata":{}},{"cell_type":"code","source":"# Set up AssemblyAI\naai.settings.api_key = \"4c5a787a36634384b228b0531ebd8c5d\"\ntranscriber = aai.Transcriber()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.858153Z","iopub.status.idle":"2025-10-07T08:03:35.858588Z","shell.execute_reply.started":"2025-10-07T08:03:35.858399Z","shell.execute_reply":"2025-10-07T08:03:35.858424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of missing files\nmissing_files = {630,636,639,727}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.860106Z","iopub.status.idle":"2025-10-07T08:03:35.860524Z","shell.execute_reply.started":"2025-10-07T08:03:35.860344Z","shell.execute_reply":"2025-10-07T08:03:35.860359Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Directory containing the audio files\naudio_directory = '/kaggle/input/911-recordings/911_recordings'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.861622Z","iopub.status.idle":"2025-10-07T08:03:35.862360Z","shell.execute_reply.started":"2025-10-07T08:03:35.861972Z","shell.execute_reply":"2025-10-07T08:03:35.861989Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List to store file names and their transcripts\ntranscripts = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.864165Z","iopub.status.idle":"2025-10-07T08:03:35.864595Z","shell.execute_reply.started":"2025-10-07T08:03:35.864373Z","shell.execute_reply":"2025-10-07T08:03:35.864393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loop through the specified range\nfor i in range(616, 747):\n    if i in missing_files:\n        continue\n\n    file_name = f\"call_{i}.mp3\"\n    file_path = os.path.join(audio_directory, file_name)\n    \n    # Ensure the file exists\n    if os.path.exists(file_path):\n        try:\n            # Transcribe the audio file\n            transcript_obj = transcriber.transcribe(file_path)\n            transcript_text = transcript_obj.text\n            transcripts.append([file_name, transcript_text])\n        except Exception as e:\n            print(f\"Error transcribing {file_name}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.865507Z","iopub.status.idle":"2025-10-07T08:03:35.866087Z","shell.execute_reply.started":"2025-10-07T08:03:35.865718Z","shell.execute_reply":"2025-10-07T08:03:35.865732Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create DataFrame\ndf = pd.DataFrame(transcripts, columns=[\"File Name\", \"Transcript\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.866936Z","iopub.status.idle":"2025-10-07T08:03:35.867323Z","shell.execute_reply.started":"2025-10-07T08:03:35.867095Z","shell.execute_reply":"2025-10-07T08:03:35.867132Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.868748Z","iopub.status.idle":"2025-10-07T08:03:35.869057Z","shell.execute_reply.started":"2025-10-07T08:03:35.868923Z","shell.execute_reply":"2025-10-07T08:03:35.868934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.870730Z","iopub.status.idle":"2025-10-07T08:03:35.871157Z","shell.execute_reply.started":"2025-10-07T08:03:35.870952Z","shell.execute_reply":"2025-10-07T08:03:35.870966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(\"911Section6.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.872018Z","iopub.status.idle":"2025-10-07T08:03:35.872425Z","shell.execute_reply.started":"2025-10-07T08:03:35.872237Z","shell.execute_reply":"2025-10-07T08:03:35.872255Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Section 1: 1 to 131 (excluding 3, 4, 5, 6, 7, 14, 22, 23, 26, 29, 41, 45, 51)","metadata":{}},{"cell_type":"code","source":"pip install assemblyai pandas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.875022Z","iopub.status.idle":"2025-10-07T08:03:35.875449Z","shell.execute_reply.started":"2025-10-07T08:03:35.875301Z","shell.execute_reply":"2025-10-07T08:03:35.875320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport assemblyai as aai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.876703Z","iopub.status.idle":"2025-10-07T08:03:35.877217Z","shell.execute_reply.started":"2025-10-07T08:03:35.876894Z","shell.execute_reply":"2025-10-07T08:03:35.876908Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up AssemblyAI\naai.settings.api_key = \"4c5a787a36634384b228b0531ebd8c5d\"\ntranscriber = aai.Transcriber()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.878462Z","iopub.status.idle":"2025-10-07T08:03:35.878758Z","shell.execute_reply.started":"2025-10-07T08:03:35.878632Z","shell.execute_reply":"2025-10-07T08:03:35.878644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of missing files\nmissing_files = {3,4,5,6,7,14,22,23,26,29,41,45,51}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.880755Z","iopub.status.idle":"2025-10-07T08:03:35.881202Z","shell.execute_reply.started":"2025-10-07T08:03:35.880987Z","shell.execute_reply":"2025-10-07T08:03:35.881002Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Directory containing the audio files\naudio_directory = '/kaggle/input/911-recordings/911_recordings'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.882888Z","iopub.status.idle":"2025-10-07T08:03:35.883490Z","shell.execute_reply.started":"2025-10-07T08:03:35.883277Z","shell.execute_reply":"2025-10-07T08:03:35.883298Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List to store file names and their transcripts\ntranscripts = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.884790Z","iopub.status.idle":"2025-10-07T08:03:35.885097Z","shell.execute_reply.started":"2025-10-07T08:03:35.884956Z","shell.execute_reply":"2025-10-07T08:03:35.884970Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Loop through the specified range\nfor i in range(1, 132):\n    if i in missing_files:\n        continue\n\n    file_name = f\"call_{i}.mp3\"\n    file_path = os.path.join(audio_directory, file_name)\n    \n    # Ensure the file exists\n    if os.path.exists(file_path):\n        try:\n            # Transcribe the audio file\n            transcript_obj = transcriber.transcribe(file_path)\n            transcript_text = transcript_obj.text\n            transcripts.append([file_name, transcript_text])\n        except Exception as e:\n            print(f\"Error transcribing {file_name}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.886563Z","iopub.status.idle":"2025-10-07T08:03:35.886837Z","shell.execute_reply.started":"2025-10-07T08:03:35.886709Z","shell.execute_reply":"2025-10-07T08:03:35.886720Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create DataFrame\ndf = pd.DataFrame(transcripts, columns=[\"File Name\", \"Transcript\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.887355Z","iopub.status.idle":"2025-10-07T08:03:35.887618Z","shell.execute_reply.started":"2025-10-07T08:03:35.887503Z","shell.execute_reply":"2025-10-07T08:03:35.887513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.888697Z","iopub.status.idle":"2025-10-07T08:03:35.889094Z","shell.execute_reply.started":"2025-10-07T08:03:35.888892Z","shell.execute_reply":"2025-10-07T08:03:35.888910Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(\"911Section1.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.890387Z","iopub.status.idle":"2025-10-07T08:03:35.890668Z","shell.execute_reply.started":"2025-10-07T08:03:35.890543Z","shell.execute_reply":"2025-10-07T08:03:35.890554Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Section 2: 132 to 252 (excluding 207, 273)","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport assemblyai as aai","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set up AssemblyAI\naai.settings.api_key = \"4c5a787a36634384b228b0531ebd8c5d\"\ntranscriber = aai.Transcriber()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of missing files\nmissing_files = {207,273}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Directory containing the audio files\naudio_directory = '/kaggle/input/911-recordings/911_recordings'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List to store file names and their transcripts\ntranscripts = []# Loop through the specified range\nfor i in range(132, 253):\n    if i in missing_files:\n        continue\n\n    file_name = f\"call_{i}.mp3\"\n    file_path = os.path.join(audio_directory, file_name)\n    \n    # Ensure the file exists\n    if os.path.exists(file_path):\n        try:\n            # Transcribe the audio file\n            transcript_obj = transcriber.transcribe(file_path)\n            transcript_text = transcript_obj.text\n            transcripts.append([file_name, transcript_text])\n        except Exception as e:\n            print(f\"Error transcribing {file_name}: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-10-07T08:03:35.899155Z","shell.execute_reply.started":"2025-10-07T08:03:35.898918Z","shell.execute_reply":"2025-10-07T08:03:35.898936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create DataFrame\ndf = pd.DataFrame(transcripts, columns=[\"File Name\", \"Transcript\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.902206Z","iopub.status.idle":"2025-10-07T08:03:35.902562Z","shell.execute_reply.started":"2025-10-07T08:03:35.902404Z","shell.execute_reply":"2025-10-07T08:03:35.902422Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.903885Z","iopub.status.idle":"2025-10-07T08:03:35.904310Z","shell.execute_reply.started":"2025-10-07T08:03:35.904160Z","shell.execute_reply":"2025-10-07T08:03:35.904179Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.906286Z","iopub.status.idle":"2025-10-07T08:03:35.906616Z","shell.execute_reply.started":"2025-10-07T08:03:35.906464Z","shell.execute_reply":"2025-10-07T08:03:35.906476Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(\"911Section2.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.908345Z","iopub.status.idle":"2025-10-07T08:03:35.908697Z","shell.execute_reply.started":"2025-10-07T08:03:35.908561Z","shell.execute_reply":"2025-10-07T08:03:35.908578Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Labeling**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n# 1. Load your CSV\ndf = pd.read_csv(\"/kaggle/input/911section3data/911Section3.csv\")  # has filename, transcription\n\n# 2. Download VADER lexicon if not already\nnltk.download(\"vader_lexicon\")\n\n# 3. Apply VADER sentiment\nsia = SentimentIntensityAnalyzer()\n\ndef get_sentiment_label(text):\n    if pd.isna(text) or text.strip() == \"\":\n        return \"neutral\"  # handle missing transcripts\n    scores = sia.polarity_scores(text)\n    compound = scores[\"compound\"]\n    if compound >= 0.05:\n        return \"positive\"\n    elif compound <= -0.05:\n        return \"negative\"\n    else:\n        return \"neutral\"\n\ndf[\"Label\"] = df[\"Transcript\"].apply(get_sentiment_label)\n\n# Save updated CSV if you want\ndf.to_csv(\"911Section3_calls_with_labels.csv\", index=False)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.909576Z","iopub.status.idle":"2025-10-07T08:03:35.909935Z","shell.execute_reply.started":"2025-10-07T08:03:35.909742Z","shell.execute_reply":"2025-10-07T08:03:35.909762Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN Model Applying","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# =========================================================\n# Load CSV with filename + label\n# =========================================================\ndf = pd.read_csv(\"/kaggle/working/911Section3_calls_with_labels.csv\")  # filename, transcription, label\n\n# Encode labels\nle = LabelEncoder()\ndf[\"label_encoded\"] = le.fit_transform(df[\"Label\"])\n\n# =========================================================\n# Dataset class\n# =========================================================\nclass AudioDataset(Dataset):\n    def __init__(self, df, audio_dir, sr=16000, max_duration=5, n_mels=64):\n        self.df = df.reset_index(drop=True)\n        self.audio_dir = audio_dir\n        self.sr = sr\n        self.max_len = sr * max_duration  # pad/truncate to fixed length\n        self.n_mels = n_mels\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        filepath = os.path.join(self.audio_dir, row[\"File Name\"])\n\n        # Load audio\n        y, sr = librosa.load(filepath, sr=self.sr)\n\n        # Pad/truncate\n        if len(y) < self.max_len:\n            y = np.pad(y, (0, self.max_len - len(y)))\n        else:\n            y = y[:self.max_len]\n\n        # Mel spectrogram\n        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=self.n_mels)\n        mel_db = librosa.power_to_db(mel, ref=np.max)\n\n        # Convert to tensor (1, H, W)\n        mel_tensor = torch.tensor(mel_db).unsqueeze(0).float()\n\n        label = torch.tensor(row[\"label_encoded\"]).long()\n        return mel_tensor, label\n\n# =========================================================\n# CNN Model with dynamic fc1 input size\n# =========================================================\nclass AudioCNN(nn.Module):\n    def __init__(self, num_classes):\n        super(AudioCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n\n        # lazy init fully connected layers\n        self.fc1 = None\n        self.fc2 = None\n        self.num_classes = num_classes\n\n    def forward_features(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = x.view(x.size(0), -1)\n\n        # initialize fc layers dynamically\n        if self.fc1 is None:\n            in_features = x.size(1)\n            self.fc1 = nn.Linear(in_features, 128).to(x.device)\n            self.fc2 = nn.Linear(128, self.num_classes).to(x.device)\n\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# =========================================================\n# Train/Test split + DataLoaders\n# =========================================================\ntrain_df, test_df = train_test_split(\n    df, test_size=0.2, stratify=df[\"label_encoded\"], random_state=42\n)\n\naudio_dir = \"/kaggle/input/911-recordings/911_recordings\"  # change to your audio folder\n\ntrain_dataset = AudioDataset(train_df, audio_dir)\ntest_dataset = AudioDataset(test_df, audio_dir)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# =========================================================\n# Compute class weights for imbalance handling\n# =========================================================\nclass_counts = train_df[\"label_encoded\"].value_counts().sort_index().to_numpy()\nclass_weights = 1.0 / class_counts\nclass_weights = class_weights / class_weights.sum()  # normalize\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n\nprint(\"Class counts:\", class_counts)\nprint(\"Class weights:\", class_weights)\n\n# =========================================================\n# Training setup\n# =========================================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AudioCNN(num_classes=len(le.classes_)).to(device)\n\n# Weighted loss\ncriterion = nn.CrossEntropyLoss(weight=class_weights_tensor.to(device))\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# =========================================================\n# Training loop\n# =========================================================\nfor epoch in range(5):  # increase epochs for real training\n    model.train()\n    total_loss = 0\n    for mel, labels in train_loader:\n        mel, labels = mel.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(mel)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader):.4f}\")\n\n# =========================================================\n# Evaluation\n# =========================================================\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for mel, labels in test_loader:\n        mel, labels = mel.to(device), labels.to(device)\n        outputs = model(mel)\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\nprint(classification_report(all_labels, all_preds, target_names=le.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.911147Z","iopub.status.idle":"2025-10-07T08:03:35.911492Z","shell.execute_reply.started":"2025-10-07T08:03:35.911352Z","shell.execute_reply":"2025-10-07T08:03:35.911365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport librosa\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# =========================================================\n# Load CSV with filename + label\n# =========================================================\ndf = pd.read_csv(\"/kaggle/working/911Section3_calls_with_labels.csv\")  # filename, transcription, label\nle = LabelEncoder()\ndf[\"label_encoded\"] = le.fit_transform(df[\"Label\"])\n\n# =========================================================\n# Audio Augmentation (for minority class)\n# =========================================================\ndef augment_audio(y, sr):\n    augmented = [y]\n    # pitch shift\n    y_pitch = librosa.effects.pitch_shift(y=y, sr=sr, n_steps=2)\n    augmented.append(y_pitch)\n    # time stretch\n    y_stretch = librosa.effects.time_stretch(y, rate=1.2)\n    augmented.append(y_stretch)\n    # add small noise\n    noise = y + 0.005 * np.random.randn(len(y))\n    augmented.append(noise)\n    return augmented\n\n# =========================================================\n# Dataset class\n# =========================================================\nclass AudioDataset(Dataset):\n    def __init__(self, df, audio_dir, sr=16000, max_duration=5, n_mels=64, augment=False):\n        self.df = df.reset_index(drop=True)\n        self.audio_dir = audio_dir\n        self.sr = sr\n        self.max_len = sr * max_duration\n        self.n_mels = n_mels\n        self.augment = augment\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        filepath = os.path.join(self.audio_dir, row[\"File Name\"])\n        y, sr = librosa.load(filepath, sr=self.sr)\n\n        # Apply augmentation for minority class\n        if self.augment and row[\"label_encoded\"] == 0:  # adjust index for your minority class\n            y_list = augment_audio(y, sr)\n            y = y_list[np.random.randint(len(y_list))]\n\n        # Pad or truncate\n        if len(y) < self.max_len:\n            y = np.pad(y, (0, self.max_len - len(y)))\n        else:\n            y = y[:self.max_len]\n\n        # Mel spectrogram\n        mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=self.n_mels)\n        mel_db = librosa.power_to_db(mel, ref=np.max)\n        mel_tensor = torch.tensor(mel_db).unsqueeze(0).float()\n\n        label = torch.tensor(row[\"label_encoded\"]).long()\n        return mel_tensor, label\n\n# =========================================================\n# CNN Model with Dropout\n# =========================================================\nclass AudioCNN(nn.Module):\n    def __init__(self, num_classes):\n        super(AudioCNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = None\n        self.fc2 = None\n        self.dropout = nn.Dropout(0.3)\n        self.num_classes = num_classes\n\n    def forward_features(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        return x\n\n    def forward(self, x):\n        x = self.forward_features(x)\n        x = x.view(x.size(0), -1)\n        if self.fc1 is None:\n            in_features = x.size(1)\n            self.fc1 = nn.Linear(in_features, 128).to(x.device)\n            self.fc2 = nn.Linear(128, self.num_classes).to(x.device)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# =========================================================\n# Train/Test split + DataLoaders\n# =========================================================\ntrain_df, test_df = train_test_split(\n    df, test_size=0.2, stratify=df[\"label_encoded\"], random_state=42\n)\naudio_dir = \"/kaggle/input/911-recordings/911_recordings\"  # change to your folder\n\ntrain_dataset = AudioDataset(train_df, audio_dir, augment=True)\ntest_dataset = AudioDataset(test_df, audio_dir, augment=False)\n\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n\n# =========================================================\n# Compute smoothed class weights\n# =========================================================\nclass_counts = train_df[\"label_encoded\"].value_counts().sort_index().to_numpy()\nclass_weights = 1.0 / class_counts\nclass_weights = np.clip(class_weights, 0.2, 0.6)  # smooth extreme values\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n\n# =========================================================\n# Training setup\n# =========================================================\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = AudioCNN(num_classes=len(le.classes_)).to(device)\ncriterion = nn.CrossEntropyLoss(weight=class_weights_tensor.to(device))\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Early stopping\nbest_val_loss = float(\"inf\")\npatience = 3\ntrigger_times = 0\n\n# =========================================================\n# Training loop with early stopping\n# =========================================================\nfor epoch in range(20):\n    model.train()\n    total_loss = 0\n    for mel, labels in train_loader:\n        mel, labels = mel.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(mel)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}\")\n\n    # Validation loss\n    model.eval()\n    val_loss = 0\n    with torch.no_grad():\n        for mel, labels in test_loader:\n            mel, labels = mel.to(device), labels.to(device)\n            outputs = model(mel)\n            loss = criterion(outputs, labels)\n            val_loss += loss.item()\n    val_loss /= len(test_loader)\n    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss:.4f}\")\n\n    # Early stopping\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        trigger_times = 0\n        torch.save(model.state_dict(), \"best_model.pth\")\n    else:\n        trigger_times += 1\n        if trigger_times >= patience:\n            print(\"Early stopping triggered!\")\n            break\n\n# =========================================================\n# Load best model and evaluate\n# =========================================================\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for mel, labels in test_loader:\n        mel, labels = mel.to(device), labels.to(device)\n        outputs = model(mel)\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\nprint(classification_report(all_labels, all_preds, target_names=le.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.912757Z","iopub.status.idle":"2025-10-07T08:03:35.913067Z","shell.execute_reply.started":"2025-10-07T08:03:35.912925Z","shell.execute_reply":"2025-10-07T08:03:35.912940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================================================\n# Load best model and evaluate\n# =========================================================\nmodel.load_state_dict(torch.load(\"best_model.pth\"))\nmodel.eval()\nall_preds, all_labels = [], []\nwith torch.no_grad():\n    for mel, labels in test_loader:\n        mel, labels = mel.to(device), labels.to(device)\n        outputs = model(mel)\n        preds = torch.argmax(outputs, dim=1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(labels.cpu().numpy())\n\nprint(classification_report(all_labels, all_preds, target_names=le.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.913818Z","iopub.status.idle":"2025-10-07T08:03:35.914211Z","shell.execute_reply.started":"2025-10-07T08:03:35.913985Z","shell.execute_reply":"2025-10-07T08:03:35.913999Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Compiling the Datasets","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport glob\n\n# Path to folder containing the 6 CSV files\npath = \"/kaggle/input/datasetpartsall\"   # change if needed\n\n# Get all CSV files in the folder\nall_files = glob.glob(path + \"/*.csv\")\n\n# Read and merge them\ndf = pd.concat((pd.read_csv(f) for f in all_files), ignore_index=True)\n\n# Save merged file\ndf.to_csv(\"combined.csv\", index=False)\n\nprint(\"Merged\", len(all_files), \"files into combined.csv\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.915460Z","iopub.status.idle":"2025-10-07T08:03:35.915870Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/working/combined.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.916577Z","iopub.status.idle":"2025-10-07T08:03:35.916838Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.918590Z","iopub.status.idle":"2025-10-07T08:03:35.918958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.920357Z","iopub.status.idle":"2025-10-07T08:03:35.920739Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Labelling","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n# 1. Load your CSV\ndf = pd.read_csv(\"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\")  # has filename, transcription\n\n# 2. Download VADER lexicon if not already\nnltk.download(\"vader_lexicon\")\n\n# 3. Apply VADER sentiment\nsia = SentimentIntensityAnalyzer()\n\ndef get_sentiment_label(text):\n    if pd.isna(text) or text.strip() == \"\":\n        return \"neutral\"  # handle missing transcripts\n    scores = sia.polarity_scores(text)\n    compound = scores[\"compound\"]\n    if compound >= 0.05:\n        return \"positive\"\n    elif compound <= -0.05:\n        return \"negative\"\n    else:\n        return \"neutral\"\n\ndf[\"Label\"] = df[\"Transcript\"].apply(get_sentiment_label)\n\n# Save updated CSV if you want\ndf.to_csv(\"911_calls_with_labels.csv\", index=False)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.922781Z","iopub.status.idle":"2025-10-07T08:03:35.923172Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Understanding the data","metadata":{}},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.924436Z","iopub.status.idle":"2025-10-07T08:03:35.925083Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.926338Z","iopub.status.idle":"2025-10-07T08:03:35.926701Z","shell.execute_reply.started":"2025-10-07T08:03:35.926579Z","shell.execute_reply":"2025-10-07T08:03:35.926590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.dropna().reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.928245Z","iopub.status.idle":"2025-10-07T08:03:35.928655Z","shell.execute_reply.started":"2025-10-07T08:03:35.928415Z","shell.execute_reply":"2025-10-07T08:03:35.928501Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.930226Z","iopub.status.idle":"2025-10-07T08:03:35.930541Z","shell.execute_reply.started":"2025-10-07T08:03:35.930407Z","shell.execute_reply":"2025-10-07T08:03:35.930419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.931904Z","iopub.status.idle":"2025-10-07T08:03:35.932280Z","shell.execute_reply.started":"2025-10-07T08:03:35.932092Z","shell.execute_reply":"2025-10-07T08:03:35.932105Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.933667Z","iopub.status.idle":"2025-10-07T08:03:35.934071Z","shell.execute_reply.started":"2025-10-07T08:03:35.933854Z","shell.execute_reply":"2025-10-07T08:03:35.933872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import pipeline\n\n# Load transformer sentiment model (DistilBERT fine-tuned on SST-2)\nsentiment_pipeline = pipeline(\"sentiment-analysis\")\n\n# Get predictions\ndf[\"Predicted_Sentiment\"] = df[\"Transcript\"].apply(lambda x: sentiment_pipeline(x[:512])[0]['label'])  \n# [:512] keeps text within model token limit\n\n# Compare\n#print(df[[\"File Name\", \"Label\", \"Predicted_Sentiment\"]])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.936475Z","iopub.status.idle":"2025-10-07T08:03:35.936758Z","shell.execute_reply.started":"2025-10-07T08:03:35.936629Z","shell.execute_reply":"2025-10-07T08:03:35.936640Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.937611Z","iopub.status.idle":"2025-10-07T08:03:35.944236Z","shell.execute_reply.started":"2025-10-07T08:03:35.937771Z","shell.execute_reply":"2025-10-07T08:03:35.937783Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['Predicted_Sentiment'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.945882Z","iopub.status.idle":"2025-10-07T08:03:35.946510Z","shell.execute_reply.started":"2025-10-07T08:03:35.946248Z","shell.execute_reply":"2025-10-07T08:03:35.946281Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.to_csv(\"911_calls_with_labels_BERT.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.948190Z","iopub.status.idle":"2025-10-07T08:03:35.948628Z","shell.execute_reply.started":"2025-10-07T08:03:35.948416Z","shell.execute_reply":"2025-10-07T08:03:35.948435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import pipeline\n\n# 1. Emotion classifier (GoEmotions)\nemotion_classifier = pipeline(\n    \"text-classification\",\n    model=\"bhadresh-savani/distilbert-base-uncased-emotion\",\n    top_k=None   # return all emotions with probabilities\n)\n\n# 2. Zero-Shot classifier (NLI)\nnli_classifier = pipeline(\"zero-shot-classification\",\n                          model=\"facebook/bart-large-mnli\")\n\nlabels = [\"urgent\", \"emergency\", \"distress\", \"calm\", \"routine\"]\n\n# Functions to apply\ndef get_emotion(text):\n    result = emotion_classifier(text[:512], top_k=None)\n    # result comes as [ [ {label:.., score:..}, {..}, ... ] ]\n    if isinstance(result, list) and isinstance(result[0], list):\n        result = result[0]\n    top = max(result, key=lambda x: x['score'])\n    return top['label']\n\n\ndef get_urgency(text):\n    result = nli_classifier(text[:512], candidate_labels=labels, multi_label=True)\n    # pick the label with the highest score\n    top_idx = result['scores'].index(max(result['scores']))\n    return result['labels'][top_idx]\n\n# Apply on dataframe\ndf[\"Predicted_Emotion\"] = df[\"Transcript\"].apply(get_emotion)\ndf[\"Predicted_Urgency\"] = df[\"Transcript\"].apply(get_urgency)\n\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.950195Z","iopub.status.idle":"2025-10-07T08:03:35.950611Z","shell.execute_reply.started":"2025-10-07T08:03:35.950413Z","shell.execute_reply":"2025-10-07T08:03:35.950430Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Using the Metadata","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.952195Z","iopub.status.idle":"2025-10-07T08:03:35.952642Z","shell.execute_reply.started":"2025-10-07T08:03:35.952423Z","shell.execute_reply":"2025-10-07T08:03:35.952443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/911-recordings/911_recordings/911_metadata.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.953891Z","iopub.status.idle":"2025-10-07T08:03:35.954346Z","shell.execute_reply.started":"2025-10-07T08:03:35.954129Z","shell.execute_reply":"2025-10-07T08:03:35.954150Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nltk.download('vader_lexicon')  # Download the VADER lexicon\nanalyzer = SentimentIntensityAnalyzer()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.956055Z","iopub.status.idle":"2025-10-07T08:03:35.956496Z","shell.execute_reply.started":"2025-10-07T08:03:35.956301Z","shell.execute_reply":"2025-10-07T08:03:35.956320Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def analyze_sentiment(text):\n    sentiment_scores = analyzer.polarity_scores(text)\n    if sentiment_scores['compound'] >= 0.05:\n        return \"Positive\"\n    elif sentiment_scores['compound'] <= -0.05:\n        return \"Negative\"\n    else:\n        return \"Neutral\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.958794Z","iopub.status.idle":"2025-10-07T08:03:35.959375Z","shell.execute_reply.started":"2025-10-07T08:03:35.959198Z","shell.execute_reply":"2025-10-07T08:03:35.959218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.960541Z","iopub.status.idle":"2025-10-07T08:03:35.960944Z","shell.execute_reply.started":"2025-10-07T08:03:35.960736Z","shell.execute_reply":"2025-10-07T08:03:35.960755Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.dropna(subset=['description'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.963142Z","iopub.status.idle":"2025-10-07T08:03:35.963562Z","shell.execute_reply.started":"2025-10-07T08:03:35.963355Z","shell.execute_reply":"2025-10-07T08:03:35.963374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.964951Z","iopub.status.idle":"2025-10-07T08:03:35.965395Z","shell.execute_reply.started":"2025-10-07T08:03:35.965193Z","shell.execute_reply":"2025-10-07T08:03:35.965212Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply sentiment analysis to the 'description' column\ndf['sentiment'] = df['description'].apply(analyze_sentiment)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.966623Z","iopub.status.idle":"2025-10-07T08:03:35.966984Z","shell.execute_reply.started":"2025-10-07T08:03:35.966814Z","shell.execute_reply":"2025-10-07T08:03:35.966832Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.967968Z","iopub.status.idle":"2025-10-07T08:03:35.968596Z","shell.execute_reply.started":"2025-10-07T08:03:35.968444Z","shell.execute_reply":"2025-10-07T08:03:35.968460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.970220Z","iopub.status.idle":"2025-10-07T08:03:35.970660Z","shell.execute_reply.started":"2025-10-07T08:03:35.970443Z","shell.execute_reply":"2025-10-07T08:03:35.970456Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.dropna(subset=['file_name'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.972093Z","iopub.status.idle":"2025-10-07T08:03:35.972545Z","shell.execute_reply.started":"2025-10-07T08:03:35.972355Z","shell.execute_reply":"2025-10-07T08:03:35.972373Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.973816Z","iopub.status.idle":"2025-10-07T08:03:35.974077Z","shell.execute_reply.started":"2025-10-07T08:03:35.973956Z","shell.execute_reply":"2025-10-07T08:03:35.973966Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.dropna(subset=['date'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.974868Z","iopub.status.idle":"2025-10-07T08:03:35.975218Z","shell.execute_reply.started":"2025-10-07T08:03:35.975067Z","shell.execute_reply":"2025-10-07T08:03:35.975080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.976496Z","iopub.status.idle":"2025-10-07T08:03:35.976756Z","shell.execute_reply.started":"2025-10-07T08:03:35.976639Z","shell.execute_reply":"2025-10-07T08:03:35.976650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.dropna(subset=['state'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.978154Z","iopub.status.idle":"2025-10-07T08:03:35.978534Z","shell.execute_reply.started":"2025-10-07T08:03:35.978342Z","shell.execute_reply":"2025-10-07T08:03:35.978358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.979726Z","iopub.status.idle":"2025-10-07T08:03:35.980062Z","shell.execute_reply.started":"2025-10-07T08:03:35.979913Z","shell.execute_reply":"2025-10-07T08:03:35.979924Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.981607Z","iopub.status.idle":"2025-10-07T08:03:35.981989Z","shell.execute_reply.started":"2025-10-07T08:03:35.981801Z","shell.execute_reply":"2025-10-07T08:03:35.981817Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['sentiment'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.983518Z","iopub.status.idle":"2025-10-07T08:03:35.983911Z","shell.execute_reply.started":"2025-10-07T08:03:35.983716Z","shell.execute_reply":"2025-10-07T08:03:35.983735Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ncolumns_to_include = [\"potential_death\", \"state\"]\n\n# Create the pair plot\nsns.pairplot(df[columns_to_include])\n\n# Display the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.985573Z","iopub.status.idle":"2025-10-07T08:03:35.985861Z","shell.execute_reply.started":"2025-10-07T08:03:35.985731Z","shell.execute_reply":"2025-10-07T08:03:35.985742Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Find the top 5 states based on the frequency of potential_death\ntop_states = df['state'].value_counts().head(5).index.tolist()\n\n# Filter the DataFrame to include only the top 5 states\nfiltered_df = df[df['state'].isin(top_states)]\n\n# Create a bar plot\nplt.figure(figsize=(10, 6))  # Adjust the figure size as needed\nsns.countplot(data=filtered_df, x='state', hue='potential_death')\n\n# Add labels and title\nplt.xlabel('State')\nplt.ylabel('Count of calls')\nplt.title('Count of Potential Death by State')\n\n# Display the plot\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.986621Z","iopub.status.idle":"2025-10-07T08:03:35.987024Z","shell.execute_reply.started":"2025-10-07T08:03:35.986809Z","shell.execute_reply":"2025-10-07T08:03:35.986826Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Assuming 'df' is your DataFrame\n\n# Find the top 15 states based on the frequency of occurrence\ntop_states = df['state'].value_counts().head(10)\n\n# Create a pie chart\nplt.figure(figsize=(8, 8))  # Adjust the figure size as needed\nplt.pie(top_states, labels=top_states.index, autopct='%1.1f%%', startangle=140)\n\n# Add a title\nplt.title('Distribution of Top 10 States by Count')\n\n# Display the pie chart\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.989502Z","iopub.status.idle":"2025-10-07T08:03:35.990621Z","shell.execute_reply.started":"2025-10-07T08:03:35.990369Z","shell.execute_reply":"2025-10-07T08:03:35.990393Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from wordcloud import WordCloud\n\n# Assuming your text data is in a column named 'text' in the DataFrame\ntext_data = \" \".join(df['title'])\n\nwordcloud = WordCloud(width=800, height=400, background_color='white', colormap='viridis', max_words=100).generate(text_data)\n\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud of Sentiment Analyzed Text')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.991719Z","iopub.status.idle":"2025-10-07T08:03:35.992173Z","shell.execute_reply.started":"2025-10-07T08:03:35.991921Z","shell.execute_reply":"2025-10-07T08:03:35.991940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Count of false alarms\nfalse_counts = df['false_alarm'].value_counts()\nprint(false_counts)\n\n# Proportion\nfalse_proportion = df['false_alarm'].mean()\nprint(f\"Proportion of false alarms: {false_proportion:.2f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.994257Z","iopub.status.idle":"2025-10-07T08:03:35.994740Z","shell.execute_reply.started":"2025-10-07T08:03:35.994505Z","shell.execute_reply":"2025-10-07T08:03:35.994525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Optional: make plots look nicer\nsns.set(style=\"whitegrid\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.997743Z","iopub.status.idle":"2025-10-07T08:03:35.998257Z","shell.execute_reply.started":"2025-10-07T08:03:35.997996Z","shell.execute_reply":"2025-10-07T08:03:35.998018Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(12,6))\nsns.countplot(data=df, x='state', hue='false_alarm')\nplt.xticks(rotation=90)\nplt.xlabel(\"State\")\nplt.ylabel(\"Number of Calls\")\nplt.title(\"False Alarms vs Real Alarms by State\")\nplt.legend(title=\"False Alarm\", labels=[\"No\", \"Yes\"])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:35.999282Z","iopub.status.idle":"2025-10-07T08:03:35.999680Z","shell.execute_reply.started":"2025-10-07T08:03:35.999482Z","shell.execute_reply":"2025-10-07T08:03:35.999500Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6,5))\nsns.countplot(data=df, x='civilian_initiated', hue='false_alarm')\nplt.xlabel(\"Civilian Initiated (0=No,1=Yes)\")\nplt.ylabel(\"Number of Calls\")\nplt.title(\"False Alarms: Civilian vs Official Initiated\")\nplt.legend(title=\"False Alarm\", labels=[\"No\", \"Yes\"])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.002006Z","iopub.status.idle":"2025-10-07T08:03:36.002444Z","shell.execute_reply.started":"2025-10-07T08:03:36.002263Z","shell.execute_reply":"2025-10-07T08:03:36.002278Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(6,5))\nsns.scatterplot(data=df, x='false_alarm', y='deaths', hue='false_alarm', s=100)\nplt.xticks([0,1], [\"No\", \"Yes\"])\nplt.ylabel(\"Number of Deaths\")\nplt.title(\"Deaths in False Alarms vs Real Alarms\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.006702Z","iopub.status.idle":"2025-10-07T08:03:36.007259Z","shell.execute_reply.started":"2025-10-07T08:03:36.006943Z","shell.execute_reply":"2025-10-07T08:03:36.006964Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pivot = df.pivot_table(values='false_alarm', index='state', columns='civilian_initiated', aggfunc='sum', fill_value=0)\n\nplt.figure(figsize=(12,6))\nsns.heatmap(pivot, annot=True, cmap=\"Reds\")\nplt.title(\"False Alarms by State and Civilian Initiated\")\nplt.xlabel(\"Civilian Initiated (0=No,1=Yes)\")\nplt.ylabel(\"State\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.009046Z","iopub.status.idle":"2025-10-07T08:03:36.009493Z","shell.execute_reply.started":"2025-10-07T08:03:36.009287Z","shell.execute_reply":"2025-10-07T08:03:36.009307Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"numeric_cols = ['civilian_initiated','deaths','potential_death']\nsns.pairplot(df, vars=numeric_cols, hue='false_alarm', palette='Set1', corner=True)\nplt.suptitle(\"Numeric Relationships by False Alarm\", y=1.02)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.010983Z","iopub.status.idle":"2025-10-07T08:03:36.011714Z","shell.execute_reply.started":"2025-10-07T08:03:36.011510Z","shell.execute_reply":"2025-10-07T08:03:36.011526Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Audio Based Handling","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# ======================\n# 1. Import Libraries\n# ======================\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.012729Z","iopub.status.idle":"2025-10-07T08:03:36.013164Z","shell.execute_reply.started":"2025-10-07T08:03:36.012941Z","shell.execute_reply":"2025-10-07T08:03:36.012961Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# 2. Paths and Parameters\n# ======================\nAUDIO_DIR = \"/kaggle/input/911-recordings/911_recordings\"\nCSV_PATH = \"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\"\nSAMPLE_RATE = 22050\nDURATION = 5   # seconds per audio clip\nSAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\nIMG_SIZE = (128, 128)  # resize spectrograms","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.014384Z","iopub.status.idle":"2025-10-07T08:03:36.014779Z","shell.execute_reply.started":"2025-10-07T08:03:36.014589Z","shell.execute_reply":"2025-10-07T08:03:36.014607Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# 3. Load Metadata\n# ======================\nmetadata = pd.read_csv(CSV_PATH)\nprint(f\"Total samples: {len(metadata)}\")\nprint(metadata['Label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.016219Z","iopub.status.idle":"2025-10-07T08:03:36.016587Z","shell.execute_reply.started":"2025-10-07T08:03:36.016395Z","shell.execute_reply":"2025-10-07T08:03:36.016414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# 4. Helper: Extract Mel Spectrogram\n# ======================\ndef extract_mel_spectrogram(file_path, sr=SAMPLE_RATE, n_mels=128):\n    y, sr = librosa.load(file_path, sr=sr)\n    \n    # Trim or pad to fixed length\n    if len(y) > SAMPLES_PER_TRACK:\n        y = y[:SAMPLES_PER_TRACK]\n    else:\n        y = np.pad(y, (0, max(0, SAMPLES_PER_TRACK - len(y))), 'constant')\n    \n    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    return mel_spec_db","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.018208Z","iopub.status.idle":"2025-10-07T08:03:36.018547Z","shell.execute_reply.started":"2025-10-07T08:03:36.018400Z","shell.execute_reply":"2025-10-07T08:03:36.018416Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# 5. Generate Dataset\n# ======================\nX, y = [], []\n\nfor i, row in metadata.iterrows():\n    file_path = os.path.join(AUDIO_DIR, row[\"File Name\"])  # <-- using 'File Name'\n    try:\n        mel_spec = extract_mel_spectrogram(file_path)\n        mel_spec = cv2.resize(mel_spec, IMG_SIZE)\n        X.append(mel_spec)\n        y.append(row[\"Label\"])  # <-- using 'Label'\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n\nX = np.array(X)[..., np.newaxis]  # add channel dim\nprint(\"Shape of X:\", X.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.020551Z","iopub.status.idle":"2025-10-07T08:03:36.020984Z","shell.execute_reply.started":"2025-10-07T08:03:36.020766Z","shell.execute_reply":"2025-10-07T08:03:36.020782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# 6. Encode Labels\n# ======================\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(y)\ny_cat = to_categorical(y_encoded)\nclasses = np.unique(y_encoded)\nprint(\"Classes:\", encoder.classes_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.038531Z","iopub.status.idle":"2025-10-07T08:03:36.039157Z","shell.execute_reply.started":"2025-10-07T08:03:36.038912Z","shell.execute_reply":"2025-10-07T08:03:36.038934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# 7. Train-Test Split\n# ======================\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_cat, test_size=0.2, stratify=y_cat, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.040168Z","iopub.status.idle":"2025-10-07T08:03:36.040510Z","shell.execute_reply.started":"2025-10-07T08:03:36.040352Z","shell.execute_reply":"2025-10-07T08:03:36.040369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# 8. Handle Imbalance with Class Weights\n# ======================\ny_integers = np.argmax(y_train, axis=1)\nclass_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_integers)\nclass_weights_dict = dict(zip(classes, class_weights))\nprint(\"Class Weights:\", class_weights_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.043084Z","iopub.status.idle":"2025-10-07T08:03:36.043774Z","shell.execute_reply.started":"2025-10-07T08:03:36.043521Z","shell.execute_reply":"2025-10-07T08:03:36.043546Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# 9. Define CNN Model\n# ======================\nmodel = Sequential([\n    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1)),\n    MaxPooling2D((2,2)),\n    Dropout(0.25),\n\n    Conv2D(64, (3,3), activation='relu'),\n    MaxPooling2D((2,2)),\n    Dropout(0.25),\n\n    Conv2D(128, (3,3), activation='relu'),\n    MaxPooling2D((2,2)),\n    Dropout(0.25),\n\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(len(classes), activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.045007Z","iopub.status.idle":"2025-10-07T08:03:36.045518Z","shell.execute_reply.started":"2025-10-07T08:03:36.045345Z","shell.execute_reply":"2025-10-07T08:03:36.045368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# 10. Train Model\n# ======================\nhistory = model.fit(\n    X_train, y_train,\n    validation_split=0.2,\n    epochs=5,\n    batch_size=32,\n    class_weight=class_weights_dict,\n    verbose=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.047831Z","iopub.status.idle":"2025-10-07T08:03:36.048347Z","shell.execute_reply.started":"2025-10-07T08:03:36.048023Z","shell.execute_reply":"2025-10-07T08:03:36.048090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# 11. Evaluate\n# ======================\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"\\n Test Accuracy: {test_acc:.4f}\")\n\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred_classes, target_names=encoder.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.049441Z","iopub.status.idle":"2025-10-07T08:03:36.049831Z","shell.execute_reply.started":"2025-10-07T08:03:36.049632Z","shell.execute_reply":"2025-10-07T08:03:36.049649Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# 12. Confusion Matrix\n# ======================\nimport seaborn as sns\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=encoder.classes_, yticklabels=encoder.classes_)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.051092Z","iopub.status.idle":"2025-10-07T08:03:36.051561Z","shell.execute_reply.started":"2025-10-07T08:03:36.051407Z","shell.execute_reply":"2025-10-07T08:03:36.051421Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ======================\n# 13. (Optional) Save Model\n# ======================\nmodel.save(\"cnn_audio_sentiment_model.h5\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:03:36.053829Z","iopub.status.idle":"2025-10-07T08:03:36.054287Z","shell.execute_reply.started":"2025-10-07T08:03:36.054042Z","shell.execute_reply":"2025-10-07T08:03:36.054062Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# CNN with Cuda","metadata":{}},{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:27:58.914156Z","iopub.execute_input":"2025-10-07T08:27:58.914373Z","iopub.status.idle":"2025-10-07T08:28:00.441355Z","shell.execute_reply.started":"2025-10-07T08:27:58.914345Z","shell.execute_reply":"2025-10-07T08:28:00.440828Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# ======================\n# 1. Import Libraries\n# ======================\nimport os\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport librosa.display\nimport matplotlib.pyplot as plt\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:28:10.389345Z","iopub.execute_input":"2025-10-07T08:28:10.390092Z","iopub.status.idle":"2025-10-07T08:28:23.930775Z","shell.execute_reply.started":"2025-10-07T08:28:10.390065Z","shell.execute_reply":"2025-10-07T08:28:23.930237Z"}},"outputs":[{"name":"stderr","text":"2025-10-07 08:28:13.059156: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759825693.252686      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759825693.312888      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ======================\n# 2. GPU CONFIGURATION\n# ======================\n# Check for GPU and print CUDA details\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        # Limit TensorFlow to use the first GPU\n        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(f\" Using GPU: {gpus[0].name}\")\n        print(f\"CUDA Detected: {tf.test.is_built_with_cuda()}\")\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\" No GPU detected. Running on CPU.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:28:31.839629Z","iopub.execute_input":"2025-10-07T08:28:31.840575Z","iopub.status.idle":"2025-10-07T08:28:32.619412Z","shell.execute_reply.started":"2025-10-07T08:28:31.840549Z","shell.execute_reply":"2025-10-07T08:28:32.618544Z"}},"outputs":[{"name":"stdout","text":" Using GPU: /physical_device:GPU:0\nCUDA Detected: True\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1759825712.614611      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ======================\n# 3. Paths and Parameters\n# ======================\nAUDIO_DIR = \"/kaggle/input/911-recordings/911_recordings\"\nCSV_PATH = \"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\"\nSAMPLE_RATE = 22050\nDURATION = 5\nSAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\nIMG_SIZE = (128, 128)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T08:28:51.625269Z","iopub.execute_input":"2025-10-07T08:28:51.625557Z","iopub.status.idle":"2025-10-07T08:28:51.629532Z","shell.execute_reply.started":"2025-10-07T08:28:51.625536Z","shell.execute_reply":"2025-10-07T08:28:51.628834Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ======================\n# 4. Load Metadata\n# ======================\nmetadata = pd.read_csv(CSV_PATH)\nprint(f\"Total samples: {len(metadata)}\")\nprint(metadata['Label'].value_counts())\nmetadata = metadata[metadata['Label'] != 'neutral'].reset_index(drop=True)\nprint(\"After dropping rare classes:\")\nprint(metadata['Label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:08:07.029393Z","iopub.execute_input":"2025-10-07T09:08:07.030255Z","iopub.status.idle":"2025-10-07T09:08:07.066847Z","shell.execute_reply.started":"2025-10-07T09:08:07.030218Z","shell.execute_reply":"2025-10-07T09:08:07.066249Z"}},"outputs":[{"name":"stdout","text":"Total samples: 706\nLabel\npositive    604\nnegative    100\nneutral       2\nName: count, dtype: int64\nAfter dropping rare classes:\nLabel\npositive    604\nnegative    100\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"# Separate classes\npositive_df = metadata[metadata['Label'] == 'positive']\nnegative_df = metadata[metadata['Label'] == 'negative']\n\n# Sample 100 from each class\npositive_sample = positive_df.sample(n=100, random_state=42)\nnegative_sample = negative_df.sample(n=100, random_state=42)\n\n# Combine into a balanced dataset\nbalanced_metadata = pd.concat([positive_sample, negative_sample]).reset_index(drop=True)\n\n# Shuffle\nmetadata = balanced_metadata.sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(\"Balanced class counts:\")\nprint(metadata['Label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:08:13.304796Z","iopub.execute_input":"2025-10-07T09:08:13.305273Z","iopub.status.idle":"2025-10-07T09:08:13.315399Z","shell.execute_reply.started":"2025-10-07T09:08:13.305248Z","shell.execute_reply":"2025-10-07T09:08:13.314678Z"}},"outputs":[{"name":"stdout","text":"Balanced class counts:\nLabel\npositive    100\nnegative    100\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":44},{"cell_type":"code","source":"# ======================\n# 5. Helper: Extract Mel Spectrogram\n# ======================\ndef extract_mel_spectrogram(file_path, sr=SAMPLE_RATE, n_mels=128):\n    y, sr = librosa.load(file_path, sr=sr)\n    # Trim/pad to fixed length\n    if len(y) > SAMPLES_PER_TRACK:\n        y = y[:SAMPLES_PER_TRACK]\n    else:\n        y = np.pad(y, (0, max(0, SAMPLES_PER_TRACK - len(y))), 'constant')\n    mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    return mel_spec_db","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:08:15.469291Z","iopub.execute_input":"2025-10-07T09:08:15.469772Z","iopub.status.idle":"2025-10-07T09:08:15.474322Z","shell.execute_reply.started":"2025-10-07T09:08:15.469747Z","shell.execute_reply":"2025-10-07T09:08:15.473577Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"# ======================\n# 6. Generate Dataset (with counter)\n# ======================\n'''X, y = [], []\n\ntotal_files = len(metadata)\nfor i, row in metadata.iterrows():\n    file_path = os.path.join(AUDIO_DIR, row[\"File Name\"])  # <-- using 'File Name'\n    try:\n        mel_spec = extract_mel_spectrogram(file_path)\n        mel_spec = cv2.resize(mel_spec, IMG_SIZE)\n        X.append(mel_spec)\n        y.append(row[\"Label\"])\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n    \n    # Progress counter\n    if (i+1) % 10 == 0 or (i+1) == total_files:\n        print(f\"Processed {i+1}/{total_files} audio files\")'''\n\n\nX, y = [], []\ntotal_files = len(metadata)\n\nfor i, row in metadata.iterrows():\n    file_path = os.path.join(AUDIO_DIR, row[\"File Name\"])\n    try:\n        mel_spec = extract_mel_spectrogram(file_path)\n        mel_spec = cv2.resize(mel_spec, IMG_SIZE)\n        X.append(mel_spec)\n        y.append(row[\"Label\"])  # <-- only append if X was successfully processed\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n\n    if (i+1) % 10 == 0 or (i+1) == total_files:\n        print(f\"Processed {i+1}/{total_files} audio files\")\n\nX = np.array(X)[..., np.newaxis]  # Add channel dim\ny = np.array(y)  # Ensure y is also a numpy array\n\nprint(\"Final dataset shapes:\", X.shape, y.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:08:17.304361Z","iopub.execute_input":"2025-10-07T09:08:17.304640Z","iopub.status.idle":"2025-10-07T09:10:44.639346Z","shell.execute_reply.started":"2025-10-07T09:08:17.304619Z","shell.execute_reply":"2025-10-07T09:10:44.638599Z"}},"outputs":[{"name":"stderr","text":"[src/libmpg123/id3.c:INT123_parse_new_id3():950] warning: ID3v2: unrealistic small tag lengh 0, skipping\n","output_type":"stream"},{"name":"stdout","text":"Processed 10/200 audio files\n","output_type":"stream"},{"name":"stderr","text":"[src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n","output_type":"stream"},{"name":"stdout","text":"Processed 20/200 audio files\nProcessed 30/200 audio files\nProcessed 40/200 audio files\nProcessed 50/200 audio files\nProcessed 60/200 audio files\n","output_type":"stream"},{"name":"stderr","text":"Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n","output_type":"stream"},{"name":"stdout","text":"Processed 70/200 audio files\nProcessed 80/200 audio files\n","output_type":"stream"},{"name":"stderr","text":"Note: Illegal Audio-MPEG-Header 0x7374616e at offset 11139840.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\n","output_type":"stream"},{"name":"stdout","text":"Processed 90/200 audio files\nProcessed 100/200 audio files\nProcessed 110/200 audio files\n","output_type":"stream"},{"name":"stderr","text":"Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n","output_type":"stream"},{"name":"stdout","text":"Processed 120/200 audio files\nProcessed 130/200 audio files\n","output_type":"stream"},{"name":"stderr","text":"[src/libmpg123/id3.c:process_extra():684] error: No extra frame text / valid description?\n[src/libmpg123/id3.c:process_extra():684] error: No extra frame text / valid description?\nWarning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n","output_type":"stream"},{"name":"stdout","text":"Processed 140/200 audio files\nProcessed 150/200 audio files\nProcessed 160/200 audio files\n","output_type":"stream"},{"name":"stderr","text":"[src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n","output_type":"stream"},{"name":"stdout","text":"Processed 170/200 audio files\nProcessed 180/200 audio files\n","output_type":"stream"},{"name":"stderr","text":"Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n","output_type":"stream"},{"name":"stdout","text":"Processed 190/200 audio files\nProcessed 200/200 audio files\nFinal dataset shapes: (200, 128, 128, 1) (200,)\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"# ======================\n# 7. Encode Labels\n# ======================\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(y)\ny_cat = to_categorical(y_encoded)\nclasses = np.unique(y_encoded)\nprint(\"Classes:\", encoder.classes_)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:12:32.749425Z","iopub.execute_input":"2025-10-07T09:12:32.749706Z","iopub.status.idle":"2025-10-07T09:12:32.755362Z","shell.execute_reply.started":"2025-10-07T09:12:32.749685Z","shell.execute_reply":"2025-10-07T09:12:32.754685Z"}},"outputs":[{"name":"stdout","text":"Classes: ['negative' 'positive']\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"# ======================\n# 8. Train-Test Split\n# ======================\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_cat, test_size=0.2, stratify=y_cat, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:12:35.294251Z","iopub.execute_input":"2025-10-07T09:12:35.294800Z","iopub.status.idle":"2025-10-07T09:12:35.304312Z","shell.execute_reply.started":"2025-10-07T09:12:35.294776Z","shell.execute_reply":"2025-10-07T09:12:35.303624Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"# ======================\n# 9. Handle Imbalance\n# ======================\ny_integers = np.argmax(y_train, axis=1)\nclass_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_integers)\nclass_weights_dict = dict(zip(classes, class_weights))\nprint(\"Class Weights:\", class_weights_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:12:37.218969Z","iopub.execute_input":"2025-10-07T09:12:37.219505Z","iopub.status.idle":"2025-10-07T09:12:37.225391Z","shell.execute_reply.started":"2025-10-07T09:12:37.219481Z","shell.execute_reply":"2025-10-07T09:12:37.224650Z"}},"outputs":[{"name":"stdout","text":"Class Weights: {0: 1.0, 1: 1.0}\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"# ======================\n# 10. Define CNN Model\n# ======================\nmodel = Sequential([\n    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE[0], IMG_SIZE[1], 1)),\n    MaxPooling2D((2,2)),\n    Dropout(0.25),\n\n    Conv2D(64, (3,3), activation='relu'),\n    MaxPooling2D((2,2)),\n    Dropout(0.25),\n\n    Conv2D(128, (3,3), activation='relu'),\n    MaxPooling2D((2,2)),\n    Dropout(0.25),\n\n    Flatten(),\n    Dense(128, activation='relu'),\n    Dropout(0.5),\n    Dense(len(classes), activation='softmax')\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:12:39.789301Z","iopub.execute_input":"2025-10-07T09:12:39.789540Z","iopub.status.idle":"2025-10-07T09:12:39.843077Z","shell.execute_reply.started":"2025-10-07T09:12:39.789522Z","shell.execute_reply":"2025-10-07T09:12:39.842484Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"# ======================\n# 11. Compile and Use GPU\n# ======================\nwith tf.device('/GPU:0'):\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    model.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:12:41.704241Z","iopub.execute_input":"2025-10-07T09:12:41.704506Z","iopub.status.idle":"2025-10-07T09:12:41.726657Z","shell.execute_reply.started":"2025-10-07T09:12:41.704485Z","shell.execute_reply":"2025-10-07T09:12:41.726125Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_3\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n\n conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)                (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)              \u001b[38;5;34m320\u001b[0m \n\n max_pooling2d_9 (\u001b[38;5;33mMaxPooling2D\u001b[0m)   (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  \u001b[38;5;34m0\u001b[0m \n\n dropout_12 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  \u001b[38;5;34m0\u001b[0m \n\n conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)             \u001b[38;5;34m18,496\u001b[0m \n\n max_pooling2d_10 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  \u001b[38;5;34m0\u001b[0m \n\n dropout_13 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  \u001b[38;5;34m0\u001b[0m \n\n conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)               (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)            \u001b[38;5;34m73,856\u001b[0m \n\n max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m \n\n dropout_14 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m0\u001b[0m \n\n flatten_3 (\u001b[38;5;33mFlatten\u001b[0m)              (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)                       \u001b[38;5;34m0\u001b[0m \n\n dense_6 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 \u001b[38;5;34m3,211,392\u001b[0m \n\n dropout_15 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n\n dense_7 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m258\u001b[0m \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n\n conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> \n\n max_pooling2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)   (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dropout_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> \n\n max_pooling2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dropout_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> \n\n max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dropout_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n flatten_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)              (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)                       <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> \n\n dropout_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,304,322\u001b[0m (12.60 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,322</span> (12.60 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,304,322\u001b[0m (12.60 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,322</span> (12.60 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"# ======================\n# 12. Train Model (GPU Accelerated)\n# ======================\nwith tf.device('/GPU:0'):\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=45,\n        batch_size=32,\n        class_weight=class_weights_dict,\n        verbose=1\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:12:43.699278Z","iopub.execute_input":"2025-10-07T09:12:43.699546Z","iopub.status.idle":"2025-10-07T09:12:55.287477Z","shell.execute_reply.started":"2025-10-07T09:12:43.699520Z","shell.execute_reply":"2025-10-07T09:12:55.286912Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 185ms/step - accuracy: 0.4365 - loss: 96.6923 - val_accuracy: 0.3750 - val_loss: 1.0944\nEpoch 2/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.5583 - loss: 5.1798 - val_accuracy: 0.4688 - val_loss: 0.6941\nEpoch 3/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.5073 - loss: 0.9336 - val_accuracy: 0.4375 - val_loss: 0.6967\nEpoch 4/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5125 - loss: 0.7729 - val_accuracy: 0.5938 - val_loss: 0.6888\nEpoch 5/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5219 - loss: 0.7055 - val_accuracy: 0.4375 - val_loss: 0.6955\nEpoch 6/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.5948 - loss: 0.6694 - val_accuracy: 0.4062 - val_loss: 0.6945\nEpoch 7/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5562 - loss: 0.6929 - val_accuracy: 0.4375 - val_loss: 0.6941\nEpoch 8/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5281 - loss: 0.6982 - val_accuracy: 0.2812 - val_loss: 0.6958\nEpoch 9/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.5604 - loss: 0.6944 - val_accuracy: 0.3125 - val_loss: 0.6991\nEpoch 10/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4625 - loss: 0.7459 - val_accuracy: 0.5312 - val_loss: 0.6933\nEpoch 11/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6031 - loss: 0.6562 - val_accuracy: 0.3750 - val_loss: 0.6944\nEpoch 12/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4844 - loss: 0.7085 - val_accuracy: 0.3750 - val_loss: 0.6945\nEpoch 13/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4823 - loss: 0.6917 - val_accuracy: 0.3750 - val_loss: 0.6949\nEpoch 14/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5094 - loss: 0.6947 - val_accuracy: 0.3750 - val_loss: 0.6950\nEpoch 15/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5583 - loss: 0.6915 - val_accuracy: 0.3750 - val_loss: 0.6953\nEpoch 16/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3646 - loss: 0.7021 - val_accuracy: 0.3750 - val_loss: 0.6955\nEpoch 17/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5229 - loss: 0.6902 - val_accuracy: 0.3750 - val_loss: 0.6957\nEpoch 18/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4698 - loss: 0.6978 - val_accuracy: 0.3750 - val_loss: 0.6960\nEpoch 19/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4490 - loss: 0.6932 - val_accuracy: 0.3750 - val_loss: 0.6963\nEpoch 20/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4958 - loss: 0.6913 - val_accuracy: 0.3750 - val_loss: 0.6967\nEpoch 21/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4708 - loss: 0.6980 - val_accuracy: 0.3750 - val_loss: 0.6971\nEpoch 22/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5240 - loss: 0.6922 - val_accuracy: 0.3750 - val_loss: 0.6977\nEpoch 23/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.6010 - loss: 0.6917 - val_accuracy: 0.3750 - val_loss: 0.6982\nEpoch 24/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4896 - loss: 0.6869 - val_accuracy: 0.3750 - val_loss: 0.6988\nEpoch 25/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4833 - loss: 0.6952 - val_accuracy: 0.3750 - val_loss: 0.6994\nEpoch 26/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5000 - loss: 0.6911 - val_accuracy: 0.3750 - val_loss: 0.6998\nEpoch 27/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5781 - loss: 0.6842 - val_accuracy: 0.3750 - val_loss: 0.7006\nEpoch 28/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5437 - loss: 0.6895 - val_accuracy: 0.3750 - val_loss: 0.7010\nEpoch 29/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5365 - loss: 0.6839 - val_accuracy: 0.3750 - val_loss: 0.7016\nEpoch 30/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5437 - loss: 0.6852 - val_accuracy: 0.3750 - val_loss: 0.7023\nEpoch 31/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5729 - loss: 0.6838 - val_accuracy: 0.3750 - val_loss: 0.7028\nEpoch 32/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5875 - loss: 0.6817 - val_accuracy: 0.3750 - val_loss: 0.7034\nEpoch 33/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.6250 - loss: 0.6698 - val_accuracy: 0.3750 - val_loss: 0.7045\nEpoch 34/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.4750 - loss: 0.6770 - val_accuracy: 0.3750 - val_loss: 0.7041\nEpoch 35/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5323 - loss: 0.6856 - val_accuracy: 0.3750 - val_loss: 0.7047\nEpoch 36/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5292 - loss: 0.6983 - val_accuracy: 0.3750 - val_loss: 0.7046\nEpoch 37/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5542 - loss: 0.6955 - val_accuracy: 0.3750 - val_loss: 0.7045\nEpoch 38/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5948 - loss: 0.6730 - val_accuracy: 0.3750 - val_loss: 0.7048\nEpoch 39/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5521 - loss: 0.6836 - val_accuracy: 0.3750 - val_loss: 0.7050\nEpoch 40/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5219 - loss: 0.6891 - val_accuracy: 0.3750 - val_loss: 0.7053\nEpoch 41/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4740 - loss: 0.6969 - val_accuracy: 0.3750 - val_loss: 0.7051\nEpoch 42/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4635 - loss: 0.6971 - val_accuracy: 0.3750 - val_loss: 0.7051\nEpoch 43/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5917 - loss: 0.6737 - val_accuracy: 0.3750 - val_loss: 0.7050\nEpoch 44/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5625 - loss: 0.6918 - val_accuracy: 0.3750 - val_loss: 0.7052\nEpoch 45/45\n\u001b[1m4/4\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5417 - loss: 0.6899 - val_accuracy: 0.3750 - val_loss: 0.7052\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"# ======================\n# 13. Evaluate\n# ======================\nwith tf.device('/GPU:0'):\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"\\n Test Accuracy: {test_acc:.4f}\")\n\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred_classes, target_names=encoder.classes_))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:12:57.849361Z","iopub.execute_input":"2025-10-07T09:12:57.850034Z","iopub.status.idle":"2025-10-07T09:12:59.593944Z","shell.execute_reply.started":"2025-10-07T09:12:57.850008Z","shell.execute_reply":"2025-10-07T09:12:59.592834Z"}},"outputs":[{"name":"stdout","text":"\n Test Accuracy: 0.5000\n\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 315ms/step\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    negative       0.00      0.00      0.00        20\n    positive       0.50      1.00      0.67        20\n\n    accuracy                           0.50        40\n   macro avg       0.25      0.50      0.33        40\nweighted avg       0.25      0.50      0.33        40\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# ======================\n# 14. Confusion Matrix\n# ======================\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(6, 5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=encoder.classes_, yticklabels=encoder.classes_)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"True\")\nplt.title(\"Confusion Matrix\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:14:24.049629Z","iopub.execute_input":"2025-10-07T09:14:24.049922Z","iopub.status.idle":"2025-10-07T09:14:24.277134Z","shell.execute_reply.started":"2025-10-07T09:14:24.049899Z","shell.execute_reply":"2025-10-07T09:14:24.276257Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 600x500 with 2 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":54},{"cell_type":"code","source":"# ======================\n# 15. Save Model\n# ======================\nmodel.save(\"cnn_audio_sentiment_model_cuda.h5\")\nprint(\" Model saved successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:14:26.179302Z","iopub.execute_input":"2025-10-07T09:14:26.179570Z","iopub.status.idle":"2025-10-07T09:14:26.281823Z","shell.execute_reply.started":"2025-10-07T09:14:26.179549Z","shell.execute_reply":"2025-10-07T09:14:26.281163Z"}},"outputs":[{"name":"stdout","text":" Model saved successfully!\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"import librosa\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\n\n# ======================\n# Parameters\n# ======================\nSAMPLE_RATE = 22050\nDURATION = 5\nSAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\nN_MFCC = 40\n\n# ======================\n# Load Metadata\n# ======================\nmetadata = pd.read_csv(\"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\")\n\n# Drop rare classes if any\nmetadata = metadata[metadata['Label'].isin(['positive','negative'])]\n\n# Optional: balance dataset\npositive_sample = metadata[metadata['Label']=='positive'].sample(n=100, random_state=42)\nnegative_sample = metadata[metadata['Label']=='negative'].sample(n=100, random_state=42)\nmetadata = pd.concat([positive_sample, negative_sample]).sample(frac=1, random_state=42)\n\n# ======================\n# Extract MFCC Features\n# ======================\ndef extract_features(file_path):\n    y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n    # Pad/trim\n    if len(y) < SAMPLES_PER_TRACK:\n        y = np.pad(y, (0, SAMPLES_PER_TRACK - len(y)))\n    else:\n        y = y[:SAMPLES_PER_TRACK]\n    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n    return np.mean(mfcc.T, axis=0)  # Take mean over time\n\nX, y_list = [], []\nfor i, row in metadata.iterrows():\n    file_path = \"/kaggle/input/911-recordings/911_recordings/\" + row[\"File Name\"]\n    try:\n        features = extract_features(file_path)\n        X.append(features)\n        y_list.append(row[\"Label\"])\n    except Exception as e:\n        print(f\"Error {file_path}: {e}\")\n\nX = np.array(X)\ny_list = np.array(y_list)\nprint(\"X shape:\", X.shape)\n\n# ======================\n# Encode labels\n# ======================\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(y_list)\ny_cat = to_categorical(y_encoded)\nclasses = np.unique(y_encoded)\nprint(\"Classes:\", encoder.classes_)\n\n# ======================\n# Train-Test Split\n# ======================\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_cat, test_size=0.2, stratify=y_cat, random_state=42\n)\n\n# ======================\n# Class Weights\n# ======================\ny_integers = np.argmax(y_train, axis=1)\nclass_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_integers)\nclass_weights_dict = dict(zip(classes, class_weights))\nprint(\"Class weights:\", class_weights_dict)\n\n# ======================\n# Build Simple ANN\n# ======================\nfrom tensorflow.keras.optimizers import Adam\n\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(N_MFCC,)),\n    Dropout(0.3),\n    Dense(64, activation='relu'),\n    Dropout(0.3),\n    Dense(len(classes), activation='softmax')\n])\n\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n\n# ======================\n# Train\n# ======================\nhistory = model.fit(\n    X_train, y_train,\n    validation_split=0.2,\n    epochs=50,\n    batch_size=16,\n    class_weight=class_weights_dict,\n    verbose=1\n)\n\n# ======================\n# Evaluate\n# ======================\ntest_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"Test Accuracy: {test_acc:.4f}\")\n\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true, y_pred_classes, target_names=encoder.classes_))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-07T09:15:17.974750Z","iopub.execute_input":"2025-10-07T09:15:17.975022Z","iopub.status.idle":"2025-10-07T09:15:30.647220Z","shell.execute_reply.started":"2025-10-07T09:15:17.974998Z","shell.execute_reply":"2025-10-07T09:15:30.646587Z"}},"outputs":[{"name":"stderr","text":"[src/libmpg123/id3.c:INT123_parse_new_id3():950] warning: ID3v2: unrealistic small tag lengh 0, skipping\n[src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\nWarning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\nWarning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n[src/libmpg123/id3.c:process_extra():684] error: No extra frame text / valid description?\n[src/libmpg123/id3.c:process_extra():684] error: No extra frame text / valid description?\nWarning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n[src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\nWarning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n","output_type":"stream"},{"name":"stdout","text":"X shape: (200, 40)\nClasses: ['negative' 'positive']\nClass weights: {0: 1.0, 1: 1.0}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_4\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n\n dense_8 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                     \u001b[38;5;34m5,248\u001b[0m \n\n dropout_16 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n\n dense_9 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                      \u001b[38;5;34m8,256\u001b[0m \n\n dropout_17 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                          \u001b[38;5;34m0\u001b[0m \n\n dense_10 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m130\u001b[0m \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n\n dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,248</span> \n\n dropout_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> \n\n dropout_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m13,634\u001b[0m (53.26 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,634</span> (53.26 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m13,634\u001b[0m (53.26 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">13,634</span> (53.26 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 62ms/step - accuracy: 0.4835 - loss: 30.9055 - val_accuracy: 0.3750 - val_loss: 21.6154\nEpoch 2/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5870 - loss: 11.9178 - val_accuracy: 0.4062 - val_loss: 15.0329\nEpoch 3/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4602 - loss: 18.7720 - val_accuracy: 0.4688 - val_loss: 4.3115\nEpoch 4/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5186 - loss: 15.7373 - val_accuracy: 0.4688 - val_loss: 3.4709\nEpoch 5/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4981 - loss: 14.7638 - val_accuracy: 0.4375 - val_loss: 5.1472\nEpoch 6/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5148 - loss: 11.7878 - val_accuracy: 0.4375 - val_loss: 3.4268\nEpoch 7/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5137 - loss: 9.6062 - val_accuracy: 0.4688 - val_loss: 2.0611\nEpoch 8/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5549 - loss: 6.7372 - val_accuracy: 0.3750 - val_loss: 4.1244\nEpoch 9/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4682 - loss: 6.3562 - val_accuracy: 0.3750 - val_loss: 3.8901\nEpoch 10/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4618 - loss: 4.6616 - val_accuracy: 0.4062 - val_loss: 1.9068\nEpoch 11/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4935 - loss: 4.3541 - val_accuracy: 0.3750 - val_loss: 1.2297\nEpoch 12/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5207 - loss: 5.8805 - val_accuracy: 0.3750 - val_loss: 1.2567\nEpoch 13/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5163 - loss: 3.1601 - val_accuracy: 0.4062 - val_loss: 1.6535\nEpoch 14/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4846 - loss: 4.2252 - val_accuracy: 0.3750 - val_loss: 1.6703\nEpoch 15/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5584 - loss: 2.2542 - val_accuracy: 0.3750 - val_loss: 1.3321\nEpoch 16/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4463 - loss: 2.7813 - val_accuracy: 0.3750 - val_loss: 1.1211\nEpoch 17/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5840 - loss: 2.0434 - val_accuracy: 0.3750 - val_loss: 1.0186\nEpoch 18/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4230 - loss: 1.5431 - val_accuracy: 0.3750 - val_loss: 0.8889\nEpoch 19/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5516 - loss: 1.7856 - val_accuracy: 0.4062 - val_loss: 0.7956\nEpoch 20/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5016 - loss: 2.8563 - val_accuracy: 0.5312 - val_loss: 0.7247\nEpoch 21/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5127 - loss: 1.3895 - val_accuracy: 0.4688 - val_loss: 0.6881\nEpoch 22/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4541 - loss: 1.0886 - val_accuracy: 0.5312 - val_loss: 0.6706\nEpoch 23/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5867 - loss: 0.7986 - val_accuracy: 0.5312 - val_loss: 0.6781\nEpoch 24/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4702 - loss: 0.9344 - val_accuracy: 0.5625 - val_loss: 0.6960\nEpoch 25/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5436 - loss: 1.4012 - val_accuracy: 0.5625 - val_loss: 0.6838\nEpoch 26/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5153 - loss: 0.9289 - val_accuracy: 0.5625 - val_loss: 0.6879\nEpoch 27/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4818 - loss: 1.3437 - val_accuracy: 0.4688 - val_loss: 0.6904\nEpoch 28/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5539 - loss: 0.7717 - val_accuracy: 0.5938 - val_loss: 0.6785\nEpoch 29/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4796 - loss: 0.9694 - val_accuracy: 0.5938 - val_loss: 0.6783\nEpoch 30/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4149 - loss: 1.2258 - val_accuracy: 0.5938 - val_loss: 0.6847\nEpoch 31/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4781 - loss: 0.8986 - val_accuracy: 0.4375 - val_loss: 0.6971\nEpoch 32/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5079 - loss: 0.8933 - val_accuracy: 0.5938 - val_loss: 0.6814\nEpoch 33/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5969 - loss: 0.7440 - val_accuracy: 0.5938 - val_loss: 0.6794\nEpoch 34/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5955 - loss: 0.9716 - val_accuracy: 0.4062 - val_loss: 0.6980\nEpoch 35/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5047 - loss: 0.8137 - val_accuracy: 0.4062 - val_loss: 0.7133\nEpoch 36/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5380 - loss: 1.0000 - val_accuracy: 0.4062 - val_loss: 0.7264\nEpoch 37/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3501 - loss: 0.8975 - val_accuracy: 0.3750 - val_loss: 0.7071\nEpoch 38/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4663 - loss: 1.0921 - val_accuracy: 0.3750 - val_loss: 0.7092\nEpoch 39/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4925 - loss: 0.7888 - val_accuracy: 0.4062 - val_loss: 0.7213\nEpoch 40/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5778 - loss: 0.7411 - val_accuracy: 0.3750 - val_loss: 0.7057\nEpoch 41/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5916 - loss: 0.7528 - val_accuracy: 0.4375 - val_loss: 0.6926\nEpoch 42/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4884 - loss: 0.8428 - val_accuracy: 0.6250 - val_loss: 0.6898\nEpoch 43/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5390 - loss: 0.7176 - val_accuracy: 0.4062 - val_loss: 0.7070\nEpoch 44/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5618 - loss: 0.6554 - val_accuracy: 0.4062 - val_loss: 0.7171\nEpoch 45/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5502 - loss: 0.7855 - val_accuracy: 0.4062 - val_loss: 0.7058\nEpoch 46/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5083 - loss: 0.7188 - val_accuracy: 0.5625 - val_loss: 0.6897\nEpoch 47/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4542 - loss: 0.7339 - val_accuracy: 0.3750 - val_loss: 0.6944\nEpoch 48/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5195 - loss: 0.7444 - val_accuracy: 0.3750 - val_loss: 0.7105\nEpoch 49/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5402 - loss: 0.6997 - val_accuracy: 0.4062 - val_loss: 0.6942\nEpoch 50/50\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6824 - loss: 0.7343 - val_accuracy: 0.6250 - val_loss: 0.6878\nTest Accuracy: 0.5500\n\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 169ms/step\n              precision    recall  f1-score   support\n\n    negative       0.53      0.80      0.64        20\n    positive       0.60      0.30      0.40        20\n\n    accuracy                           0.55        40\n   macro avg       0.57      0.55      0.52        40\nweighted avg       0.57      0.55      0.52        40\n\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"# Improving the ANN which performed way better than CNN","metadata":{}},{"cell_type":"code","source":"metadata.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:37:54.640307Z","iopub.execute_input":"2025-10-08T04:37:54.640956Z","iopub.status.idle":"2025-10-08T04:37:54.664535Z","shell.execute_reply.started":"2025-10-08T04:37:54.640930Z","shell.execute_reply":"2025-10-08T04:37:54.664009Z"}},"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"      File Name                                         Transcript     Label\n0  call_598.mp3  Dispatch. Rebecca, hi. Sure. Sir, please. Just...  positive\n1  call_624.mp3  91 1. Do you need police, fire, medical? Yes. ...  positive\n2    call_2.mp3  Monday 2202006 at 5:59pm Emergency 91 1. Where...  negative\n3    call_1.mp3  Officers, 69 Riverside Drive, Iowa. Transporti...  negative\n4   call_24.mp3  239, hold here. What's your location? 1331 Ban...  negative","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>File Name</th>\n      <th>Transcript</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>call_598.mp3</td>\n      <td>Dispatch. Rebecca, hi. Sure. Sir, please. Just...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>call_624.mp3</td>\n      <td>91 1. Do you need police, fire, medical? Yes. ...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>call_2.mp3</td>\n      <td>Monday 2202006 at 5:59pm Emergency 91 1. Where...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>call_1.mp3</td>\n      <td>Officers, 69 Riverside Drive, Iowa. Transporti...</td>\n      <td>negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>call_24.mp3</td>\n      <td>239, hold here. What's your location? 1331 Ban...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":29},{"cell_type":"code","source":"metadata['Label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:40:40.266363Z","iopub.execute_input":"2025-10-08T04:40:40.266898Z","iopub.status.idle":"2025-10-08T04:40:40.272291Z","shell.execute_reply.started":"2025-10-08T04:40:40.266875Z","shell.execute_reply":"2025-10-08T04:40:40.271581Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"Series([], Name: count, dtype: int64)"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"# Swap labels\nmetadata['Label'] = metadata['Label'].map({'positive': 'Negative', 'negative': 'Positive'})\n\n# Verify\nprint(metadata['Label'].value_counts())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:40:47.967585Z","iopub.execute_input":"2025-10-08T04:40:47.968274Z","iopub.status.idle":"2025-10-08T04:40:47.974082Z","shell.execute_reply.started":"2025-10-08T04:40:47.968250Z","shell.execute_reply":"2025-10-08T04:40:47.973199Z"}},"outputs":[{"name":"stdout","text":"Label\nNegative    604\nPositive    100\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":37},{"cell_type":"code","source":"AUDIO_DIR = \"/kaggle/input/911-recordings/911_recordings\"\nCSV_PATH = \"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\"\nSAMPLE_RATE = 22050\nDURATION = 5\nSAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\nN_MFCC = 20\n\n# ======================\n# 4. Load Metadata and Balance Dataset\n# ======================\nmetadata = pd.read_csv(CSV_PATH)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:40:42.646211Z","iopub.execute_input":"2025-10-08T04:40:42.646909Z","iopub.status.idle":"2025-10-08T04:40:42.684563Z","shell.execute_reply.started":"2025-10-08T04:40:42.646881Z","shell.execute_reply":"2025-10-08T04:40:42.684008Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# ======================\n# 1. Import Libraries\n# ======================\nimport os\nimport numpy as np\nimport pandas as pd\nimport librosa\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import classification_report\n\n# ======================\n# 2. GPU Configuration\n# ======================\nimport tensorflow as tf\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n        print(f\" Using GPU: {gpus[0].name}\")\n        print(f\"CUDA Detected: {tf.test.is_built_with_cuda()}\")\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\" No GPU detected. Running on CPU.\")\n\n# ======================\n# 3. Paths and Parameters\n# ======================\nAUDIO_DIR = \"/kaggle/input/911-recordings/911_recordings\"\nCSV_PATH = \"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\"\nSAMPLE_RATE = 22050\nDURATION = 5\nSAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\nN_MFCC = 20\n\n# ======================\n# 4. Load Metadata and Balance Dataset\n# ======================\nmetadata = pd.read_csv(CSV_PATH)\n\n# Drop rare/neutral classes\nmetadata = metadata[metadata['Label'].isin(['positive','negative'])]\n\n#metadata['Label'] = metadata['Label'].map({'positive': 'Negative', 'negative': 'Positive'})\n\n# Balance: sample 100 positive, 100 negative\npositive_sample = metadata[metadata['Label']=='positive'].sample(n=90, random_state=42)\nnegative_sample = metadata[metadata['Label']=='negative'].sample(n=100, random_state=42)\nmetadata = pd.concat([positive_sample, negative_sample]).sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(\"Balanced class counts:\")\nprint(metadata['Label'].value_counts())\n\n# ======================?\n# 5. Feature Extraction\n# ======================\ndef extract_features(file_path):\n    y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n    # Pad/trim\n    if len(y) < SAMPLES_PER_TRACK:\n        y = np.pad(y, (0, SAMPLES_PER_TRACK - len(y)))\n    else:\n        y = y[:SAMPLES_PER_TRACK]\n    \n    # MFCC\n    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n    mfccs_mean = np.mean(mfccs.T, axis=0)\n    \n    # Chroma\n    stft = np.abs(librosa.stft(y))\n    chroma = librosa.feature.chroma_stft(S=stft, sr=sr)\n    chroma_mean = np.mean(chroma.T, axis=0)\n    \n    # Spectral Contrast\n    spec_contrast = librosa.feature.spectral_contrast(S=stft, sr=sr)\n    spec_contrast_mean = np.mean(spec_contrast.T, axis=0)\n    \n    # Combine features\n    features = np.concatenate([mfccs_mean, chroma_mean, spec_contrast_mean])\n    return features\n\n# Process audio files\nX, y_list = [], []\ntotal_files = len(metadata)\nfor i, row in metadata.iterrows():\n    file_path = os.path.join(AUDIO_DIR, row[\"File Name\"])\n    try:\n        features = extract_features(file_path)\n        X.append(features)\n        y_list.append(row[\"Label\"])\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n    \n    if (i+1) % 10 == 0 or (i+1) == total_files:\n        print(f\"Processed {i+1}/{total_files} audio files\")\n\nX = np.array(X)\ny_list = np.array(y_list)\nprint(\"Final feature shape:\", X.shape)\n\n# ======================\n# 6. Encode Labels\n# ======================\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(y_list)\ny_cat = to_categorical(y_encoded)\nclasses = np.unique(y_encoded)\nprint(\"Classes:\", encoder.classes_)\n\n# ======================\n# 7. Train-Test Split\n# ======================\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_cat, test_size=0.2, stratify=y_cat, random_state=42\n)\nprint(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n\n# ======================\n# 8. Class Weights\n# ======================\ny_integers = np.argmax(y_train, axis=1)\nclass_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_integers)\nclass_weights_dict = dict(zip(classes, class_weights))\nprint(\"Class weights:\", class_weights_dict)\n\n# ======================\n# 9. Build ANN Model\n# ======================\nmodel = Sequential([\n    Dense(256, activation='relu', input_shape=(X.shape[1],)),\n    Dropout(0.3),\n    Dense(128, activation='relu'),\n    Dropout(0.3),\n    Dense(len(classes), activation='softmax')\n])\n\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n\n# ======================\n# 10. Train Model\n# ======================\nwith tf.device('/GPU:0'):\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=45,\n        batch_size=16,\n        #class_weight=class_weights_dict,\n        verbose=1\n    )\n\n# ======================\n# 11. Evaluate\n# ======================\nwith tf.device('/GPU:0'):\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"\\n Test Accuracy: {test_acc:.4f}\")\n\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred_classes, target_names=encoder.classes_))\n\n# ======================\n# 11b. Confusion Matrix\n# ======================\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n# ======================\n# 12. Save Model\n# ======================\nmodel.save(\"ann_audio_sentiment_model.h5\")\nprint(\" Model saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:50:58.519300Z","iopub.execute_input":"2025-10-08T04:50:58.519889Z","iopub.status.idle":"2025-10-08T04:51:14.732556Z","shell.execute_reply.started":"2025-10-08T04:50:58.519865Z","shell.execute_reply":"2025-10-08T04:51:14.731760Z"}},"outputs":[{"name":"stdout","text":" Using GPU: /physical_device:GPU:0\nCUDA Detected: True\nBalanced class counts:\nLabel\nnegative    100\npositive     90\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n","output_type":"stream"},{"name":"stdout","text":"Processed 10/190 audio files\nProcessed 20/190 audio files\nProcessed 30/190 audio files\nProcessed 40/190 audio files\n","output_type":"stream"},{"name":"stderr","text":"[src/libmpg123/id3.c:INT123_parse_new_id3():950] warning: ID3v2: unrealistic small tag lengh 0, skipping\nWarning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n","output_type":"stream"},{"name":"stdout","text":"Processed 50/190 audio files\nProcessed 60/190 audio files\nProcessed 70/190 audio files\nProcessed 80/190 audio files\nProcessed 90/190 audio files\n","output_type":"stream"},{"name":"stderr","text":"Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n","output_type":"stream"},{"name":"stdout","text":"Processed 100/190 audio files\n","output_type":"stream"},{"name":"stderr","text":"[src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n","output_type":"stream"},{"name":"stdout","text":"Processed 110/190 audio files\nProcessed 120/190 audio files\nProcessed 130/190 audio files\nProcessed 140/190 audio files\nProcessed 150/190 audio files\nProcessed 160/190 audio files\n","output_type":"stream"},{"name":"stderr","text":"[src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n","output_type":"stream"},{"name":"stdout","text":"Processed 170/190 audio files\n","output_type":"stream"},{"name":"stderr","text":"Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n","output_type":"stream"},{"name":"stdout","text":"Processed 180/190 audio files\nProcessed 190/190 audio files\nFinal feature shape: (190, 39)\nClasses: ['negative' 'positive']\nTrain/Test shapes: (152, 39) (38, 39)\nClass weights: {0: 0.95, 1: 1.0555555555555556}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_25\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_25\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n\n dense_75 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m10,240\u001b[0m \n\n dropout_50 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n\n dense_76 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m32,896\u001b[0m \n\n dropout_51 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n\n dense_77 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m258\u001b[0m \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n\n dense_75 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,240</span> \n\n dropout_50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dense_76 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> \n\n dropout_51 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dense_77 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m43,394\u001b[0m (169.51 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,394</span> (169.51 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m43,394\u001b[0m (169.51 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,394</span> (169.51 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 219ms/step - accuracy: 0.5138 - loss: 9.6751 - val_accuracy: 0.3548 - val_loss: 9.3923\nEpoch 2/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5758 - loss: 12.0098 - val_accuracy: 0.6452 - val_loss: 4.5341\nEpoch 3/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4503 - loss: 11.4765 - val_accuracy: 0.3548 - val_loss: 8.6201\nEpoch 4/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6031 - loss: 7.6916 - val_accuracy: 0.5161 - val_loss: 2.6401\nEpoch 5/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3681 - loss: 13.8077 - val_accuracy: 0.4194 - val_loss: 3.5371\nEpoch 6/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5366 - loss: 6.0866 - val_accuracy: 0.3548 - val_loss: 7.9869\nEpoch 7/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4701 - loss: 8.0091 - val_accuracy: 0.5484 - val_loss: 1.8817\nEpoch 8/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5187 - loss: 6.4701 - val_accuracy: 0.3871 - val_loss: 3.5024\nEpoch 9/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4326 - loss: 6.7637 - val_accuracy: 0.5806 - val_loss: 1.0452\nEpoch 10/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5024 - loss: 4.1784 - val_accuracy: 0.3548 - val_loss: 7.2470\nEpoch 11/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5455 - loss: 5.3762 - val_accuracy: 0.6452 - val_loss: 1.1842\nEpoch 12/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5679 - loss: 3.5350 - val_accuracy: 0.4194 - val_loss: 1.9017\nEpoch 13/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6211 - loss: 2.7253 - val_accuracy: 0.3548 - val_loss: 2.7456\nEpoch 14/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4450 - loss: 3.6930 - val_accuracy: 0.6129 - val_loss: 0.8192\nEpoch 15/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5334 - loss: 2.4887 - val_accuracy: 0.3871 - val_loss: 1.7110\nEpoch 16/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - loss: 2.6881 - val_accuracy: 0.3871 - val_loss: 1.6515\nEpoch 17/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5629 - loss: 1.8038 - val_accuracy: 0.3871 - val_loss: 1.2534\nEpoch 18/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5814 - loss: 1.5801 - val_accuracy: 0.3871 - val_loss: 0.9101\nEpoch 19/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5512 - loss: 2.0307 - val_accuracy: 0.3548 - val_loss: 1.1670\nEpoch 20/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5527 - loss: 1.6901 - val_accuracy: 0.3548 - val_loss: 0.9148\nEpoch 21/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5469 - loss: 1.3135 - val_accuracy: 0.3548 - val_loss: 1.1203\nEpoch 22/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6777 - loss: 1.2150 - val_accuracy: 0.3548 - val_loss: 1.2394\nEpoch 23/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5604 - loss: 1.5825 - val_accuracy: 0.3548 - val_loss: 1.0451\nEpoch 24/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4508 - loss: 1.2942 - val_accuracy: 0.4194 - val_loss: 0.8212\nEpoch 25/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5039 - loss: 1.3128 - val_accuracy: 0.5161 - val_loss: 0.8019\nEpoch 26/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5728 - loss: 0.9874 - val_accuracy: 0.4839 - val_loss: 0.7625\nEpoch 27/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5227 - loss: 0.9420 - val_accuracy: 0.4194 - val_loss: 0.8124\nEpoch 28/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4776 - loss: 0.9628 - val_accuracy: 0.3871 - val_loss: 0.7696\nEpoch 29/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6429 - loss: 0.9357 - val_accuracy: 0.3871 - val_loss: 0.8300\nEpoch 30/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6016 - loss: 0.8203 - val_accuracy: 0.3871 - val_loss: 0.8730\nEpoch 31/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5939 - loss: 0.9652 - val_accuracy: 0.3871 - val_loss: 0.8143\nEpoch 32/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5154 - loss: 0.9258 - val_accuracy: 0.4194 - val_loss: 0.7814\nEpoch 33/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.4932 - loss: 1.1419 - val_accuracy: 0.4839 - val_loss: 0.7369\nEpoch 34/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5550 - loss: 0.8074 - val_accuracy: 0.5484 - val_loss: 0.7189\nEpoch 35/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5079 - loss: 0.8477 - val_accuracy: 0.5484 - val_loss: 0.7165\nEpoch 36/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5022 - loss: 1.0005 - val_accuracy: 0.3871 - val_loss: 0.7374\nEpoch 37/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4956 - loss: 0.8653 - val_accuracy: 0.3548 - val_loss: 0.7603\nEpoch 38/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5243 - loss: 0.9696 - val_accuracy: 0.3548 - val_loss: 0.7501\nEpoch 39/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6434 - loss: 0.7149 - val_accuracy: 0.3548 - val_loss: 0.7429\nEpoch 40/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4942 - loss: 0.8044 - val_accuracy: 0.4839 - val_loss: 0.7103\nEpoch 41/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4992 - loss: 0.7290 - val_accuracy: 0.4194 - val_loss: 0.7084\nEpoch 42/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5652 - loss: 0.7210 - val_accuracy: 0.4516 - val_loss: 0.7097\nEpoch 43/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5517 - loss: 0.8899 - val_accuracy: 0.6129 - val_loss: 0.7147\nEpoch 44/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5689 - loss: 0.7345 - val_accuracy: 0.6452 - val_loss: 0.7647\nEpoch 45/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5434 - loss: 0.8520 - val_accuracy: 0.7097 - val_loss: 0.7939\n\n Test Accuracy: 0.5526\n\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    negative       0.58      0.55      0.56        20\n    positive       0.53      0.56      0.54        18\n\n    accuracy                           0.55        38\n   macro avg       0.55      0.55      0.55        38\nweighted avg       0.55      0.55      0.55        38\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x500 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":" Model saved successfully!\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# trying to avaoid missclassification","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport soundfile as sf\nimport IPython.display as ipd\nimport librosa\nimport pandas as pd\nimport seaborn as sns\nimport glob\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T03:44:57.776675Z","iopub.execute_input":"2025-10-08T03:44:57.776832Z","iopub.status.idle":"2025-10-08T03:45:02.507686Z","shell.execute_reply.started":"2025-10-08T03:44:57.776816Z","shell.execute_reply":"2025-10-08T03:45:02.506955Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# =======================\n#  IMPORTS\n# =======================\nimport os\nimport librosa\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.metrics import classification_report\nimport joblib","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T03:45:32.348059Z","iopub.execute_input":"2025-10-08T03:45:32.348378Z","iopub.status.idle":"2025-10-08T03:45:32.766941Z","shell.execute_reply.started":"2025-10-08T03:45:32.348354Z","shell.execute_reply":"2025-10-08T03:45:32.766431Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# =======================\n#  FEATURE EXTRACTION\n# =======================\ndef extract_features(audio, sr):\n    \"\"\"\n    Extract MFCC, Zero Crossing Rate, RMS, Spectral Centroid, Bandwidth,\n    Contrast, and Polynomial coefficients from audio.\n    Returns a 1D numpy array of combined features.\n    \"\"\"\n    mfcc = np.mean(librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40).T, axis=0)\n    zero = np.mean(librosa.feature.zero_crossing_rate(y=audio).T, axis=0)\n    rms = np.mean(librosa.feature.rms(y=audio).T, axis=0)\n    sc = np.mean(librosa.feature.spectral_centroid(y=audio, sr=sr).T, axis=0)\n    sb = np.mean(librosa.feature.spectral_bandwidth(y=audio, sr=sr).T, axis=0)\n    sco = np.mean(librosa.feature.spectral_contrast(y=audio, sr=sr).T, axis=0)\n    poly = np.mean(librosa.feature.poly_features(y=audio, sr=sr, order=2).T, axis=0)\n\n    # Combine everything\n    combined = np.hstack([mfcc, zero, rms, sc, sb, sco, poly])\n    return combined","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T03:45:35.552287Z","iopub.execute_input":"2025-10-08T03:45:35.552986Z","iopub.status.idle":"2025-10-08T03:45:35.558759Z","shell.execute_reply.started":"2025-10-08T03:45:35.552962Z","shell.execute_reply":"2025-10-08T03:45:35.558061Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# =======================\n#  DATASET CREATION\n# =======================\ndef process_subdirectories(root_directory):\n    \"\"\"\n    Walks through all subdirectories and extracts features.\n    Assumes directory structure: root/label_name/audio.wav\n    \"\"\"\n    features, labels, files = [], [], []\n\n    print(\" Extracting features...\")\n    for root, _, files_in_dir in os.walk(root_directory):\n        for file in tqdm(files_in_dir):\n            if file.endswith(\".wav\"):\n                file_path = os.path.join(root, file)\n                try:\n                    audio, sr = librosa.load(file_path, sr=None)\n                    feat = extract_features(audio, sr)\n                    label = os.path.basename(root)\n                    features.append(feat)\n                    labels.append(label)\n                    files.append(file_path)\n                except Exception as e:\n                    print(f\" Error processing {file_path}: {e}\")\n\n    df = pd.DataFrame(features)\n    df[\"Label\"] = labels\n    df[\"File_Path\"] = files\n    print(\" Feature extraction complete!\")\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T03:45:36.998082Z","iopub.execute_input":"2025-10-08T03:45:36.998646Z","iopub.status.idle":"2025-10-08T03:45:37.004207Z","shell.execute_reply.started":"2025-10-08T03:45:36.998620Z","shell.execute_reply":"2025-10-08T03:45:37.003546Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# =======================\n#  MODEL TRAINING (GPU)\n# =======================\ndef train_xgboost_model(df):\n    from xgboost import XGBClassifier\n\n    X = df.drop(columns=[\"Label\", \"File_Path\"])\n    y = df[\"Label\"]\n\n    # Encode labels\n    le = LabelEncoder()\n    y_encoded = le.fit_transform(y)\n\n    # Normalize\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n    )\n\n    print(\" Training model on GPU...\")\n    model = XGBClassifier(\n        tree_method=\"gpu_hist\",  # Enables GPU acceleration\n        predictor=\"gpu_predictor\",\n        n_estimators=200,\n        learning_rate=0.05,\n        max_depth=8,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        eval_metric=\"mlogloss\"\n    )\n\n    model.fit(X_train, y_train)\n\n    y_pred = model.predict(X_test)\n    print(\"\\n Classification Report:\")\n    print(classification_report(y_test, y_pred, target_names=le.classes_))\n\n    # Save model and encoders\n    joblib.dump(model, \"audio_classifier_xgb.pkl\")\n    joblib.dump(scaler, \"scaler.pkl\")\n    joblib.dump(le, \"label_encoder.pkl\")\n\n    print(\"\\n Model, Scaler, and Label Encoder saved successfully!\")\n    return model, le, scaler","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T03:45:41.393853Z","iopub.execute_input":"2025-10-08T03:45:41.394559Z","iopub.status.idle":"2025-10-08T03:45:41.400251Z","shell.execute_reply.started":"2025-10-08T03:45:41.394534Z","shell.execute_reply":"2025-10-08T03:45:41.399653Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# =======================\n#  MAIN PIPELINE\n# =======================\nif __name__ == \"__main__\":\n    # Path to dataset directory\n    dataset_dir = \"/kaggle/input/911-recordings/911_recordings\"\n\n    df = process_subdirectories(dataset_dir)\n    df.to_csv(\"audio_features.csv\", index=False)\n    print(f\" Features saved to audio_features.csv ({df.shape})\")\n\n    model, le, scaler = train_xgboost_model(df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T03:45:44.246101Z","iopub.execute_input":"2025-10-08T03:45:44.246797Z","iopub.status.idle":"2025-10-08T03:45:46.371040Z","shell.execute_reply.started":"2025-10-08T03:45:44.246773Z","shell.execute_reply":"2025-10-08T03:45:46.369856Z"}},"outputs":[{"name":"stdout","text":" Extracting features...\n","output_type":"stream"},{"name":"stderr","text":"100%|| 708/708 [00:00<00:00, 2436068.28it/s]\n","output_type":"stream"},{"name":"stdout","text":" Feature extraction complete!\n Features saved to audio_features.csv ((0, 2))\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3072205788.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" Features saved to audio_features.csv ({df.shape})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_xgboost_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_36/3944881748.py\u001b[0m in \u001b[0;36mtrain_xgboost_model\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mX_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Split data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 878\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    879\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    859\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m         \u001b[0mfirst_call\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 861\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m    862\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation should be done on X, y or both.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 565\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    566\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    776\u001b[0m         )\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mdtype_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iloc\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: at least one array or dtype is required"],"ename":"ValueError","evalue":"at least one array or dtype is required","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"import os\nimport librosa\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\n\n# -----------------------\n# Feature Extraction\n# -----------------------\n\ndef extract_features_from_audio(file_path, sr=22050):\n    try:\n        y, sr = librosa.load(file_path, sr=sr)\n        if len(y) == 0:\n            return None\n\n        # Extract multiple features\n        mfcc = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13).T, axis=0)\n        spectral_centroid = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n        chroma = np.mean(librosa.feature.chroma_stft(y=y, sr=sr))\n        zcr = np.mean(librosa.feature.zero_crossing_rate(y))\n        rms = np.mean(librosa.feature.rms(y=y))\n        tempo, _ = librosa.beat.beat_track(y=y, sr=sr)\n\n        features = np.hstack([mfcc, spectral_centroid, chroma, zcr, rms, tempo])\n        return features\n\n    except Exception as e:\n        print(f\" Error processing {file_path}: {e}\")\n        return None\n\ndef build_feature_dataset(audio_dir):\n    data = []\n    labels = []\n\n    for root, _, files in os.walk(audio_dir):\n        for f in tqdm(files, desc=\" Extracting features\"):\n            if f.lower().endswith((\".mp3\", \".wav\", \".flac\", \".ogg\", \".m4a\")):\n                path = os.path.join(root, f)\n                label = os.path.basename(root)\n                features = extract_features_from_audio(path)\n                if features is not None:\n                    data.append(features)\n                    labels.append(label)\n\n    df = pd.DataFrame(data)\n    df[\"label\"] = labels\n    return df\n\n# -----------------------\n# Model Training\n# -----------------------\n\ndef train_xgboost_model(df):\n    if df.empty or \"label\" not in df.columns:\n        raise ValueError(\" Feature DataFrame is empty or missing label column.\")\n\n    X = df.drop(\"label\", axis=1)\n    y = df[\"label\"]\n\n    # Encode labels\n    le = LabelEncoder()\n    y_encoded = le.fit_transform(y)\n\n    # Scale features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n\n    # Train XGBoost model\n    model = XGBClassifier(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=8,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        tree_method=\"gpu_hist\",  #  GPU acceleration\n        use_label_encoder=False,\n        eval_metric=\"mlogloss\"\n    )\n    model.fit(X_train, y_train)\n\n    # Evaluate\n    preds = model.predict(X_test)\n    print(\" Model Training Complete!\")\n    print(classification_report(y_test, preds, target_names=le.classes_))\n\n    return model, le, scaler\n\n# -----------------------\n# Run Everything\n# -----------------------\n\nif __name__ == \"__main__\":\n    AUDIO_DIR = \"/kaggle/input/911-recordings/911_recordings\"  # e.g. \"dataset/train\"\n\n    df = build_feature_dataset(AUDIO_DIR)\n    print(f\" Features saved  shape: {df.shape}\")\n    df.to_csv(\"audio_features.csv\", index=False)\n\n    model, le, scaler = train_xgboost_model(df)\n    print(\" Model ready for predictions!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T03:47:31.323846Z","iopub.execute_input":"2025-10-08T03:47:31.324577Z","iopub.status.idle":"2025-10-08T03:51:20.404143Z","shell.execute_reply.started":"2025-10-08T03:47:31.324551Z","shell.execute_reply":"2025-10-08T03:51:20.402991Z"}},"outputs":[{"name":"stderr","text":" Extracting features:   4%|         | 27/708 [02:47<42:07,  3.71s/it]  [src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n Extracting features:   5%|         | 33/708 [03:22<1:07:51,  6.03s/it]Note: Illegal Audio-MPEG-Header 0x7374616e at offset 11139840.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\n Extracting features:   5%|         | 38/708 [03:48<1:07:04,  6.01s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1941697380.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mAUDIO_DIR\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/911-recordings/911_recordings\"\u001b[0m  \u001b[0;31m# e.g. \"dataset/train\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_feature_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAUDIO_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" Features saved  shape: {df.shape}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"audio_features.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1941697380.py\u001b[0m in \u001b[0;36mbuild_feature_dataset\u001b[0;34m(audio_dir)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features_from_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1941697380.py\u001b[0m in \u001b[0;36mextract_features_from_audio\u001b[0;34m(file_path, sr)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mzcr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_crossing_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mrms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mtempo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeat_track\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmfcc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspectral_centroid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchroma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzcr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtempo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/beat.py\u001b[0m in \u001b[0;36mbeat_track\u001b[0;34m(y, sr, onset_envelope, hop_length, start_bpm, tightness, trim, bpm, prior, units, sparse)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0;31m# Estimate BPM if one was not provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbpm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         bpm = _tempo(\n\u001b[0m\u001b[1;32m    237\u001b[0m             \u001b[0monset_envelope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0monset_envelope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/feature/rhythm.py\u001b[0m in \u001b[0;36mtempo\u001b[0;34m(y, sr, onset_envelope, tg, hop_length, start_bpm, std_bpm, ac_size, max_tempo, aggregate, prior)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mwin_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_to_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mac_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhop_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhop_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m         tg = tempogram(\n\u001b[0m\u001b[1;32m    416\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/feature/rhythm.py\u001b[0m in \u001b[0;36mtempogram\u001b[0;34m(y, sr, onset_envelope, hop_length, win_length, center, window, norm)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[0;31m# Window, autocorrelate, and normalize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m     return util.normalize(\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0mautocorrelate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0modf_frame\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mac_window\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py\u001b[0m in \u001b[0;36mautocorrelate\u001b[0;34m(y, max_size, axis)\u001b[0m\n\u001b[1;32m    923\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0;31m# Compute the power spectrum along the chosen axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m         \u001b[0mpowspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrfft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_pad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;31m# Convert back to time domain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/fft/_backend.py\u001b[0m in \u001b[0;36m__ua_function__\u001b[0;34m(method, args, kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/fft/_basic_backend.py\u001b[0m in \u001b[0;36mrfft\u001b[0;34m(x, n, axis, norm, overwrite_x, workers, plan)\u001b[0m\n\u001b[1;32m     89\u001b[0m def rfft(x, n=None, axis=-1, norm=None,\n\u001b[1;32m     90\u001b[0m          overwrite_x=False, workers=None, *, plan=None):\n\u001b[0;32m---> 91\u001b[0;31m     return _execute_1D('rfft', _pocketfft.rfft, x, n=n, axis=axis, norm=norm,\n\u001b[0m\u001b[1;32m     92\u001b[0m                        overwrite_x=overwrite_x, workers=workers, plan=plan)\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/fft/_basic_backend.py\u001b[0m in \u001b[0;36m_execute_1D\u001b[0;34m(func_str, pocketfft_func, x, n, axis, norm, overwrite_x, workers, plan)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         return pocketfft_func(x, n=n, axis=axis, norm=norm,\n\u001b[0m\u001b[1;32m     33\u001b[0m                               overwrite_x=overwrite_x, workers=workers, plan=plan)\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/scipy/fft/_pocketfft/basic.py\u001b[0m in \u001b[0;36mr2c\u001b[0;34m(forward, x, n, axis, norm, overwrite_x, workers, plan)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Note: overwrite_x is not utilised\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpfft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr2c\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"import os\nimport librosa\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import classification_report\n\n# -----------------------------\n# 1  Feature Extraction\n# -----------------------------\ndef extract_mfcc_features(file_path, sr=22050, n_mfcc=40):\n    try:\n        y, sr = librosa.load(file_path, sr=sr)\n        if len(y) == 0:\n            return None\n\n        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n        mfcc_scaled = np.mean(mfcc.T, axis=0)  # Average over time\n        return mfcc_scaled\n    except Exception as e:\n        print(f\" Error processing {file_path}: {e}\")\n        return None\n\ndef build_mfcc_dataset(audio_dir):\n    records = []\n\n    for root, _, files in os.walk(audio_dir):\n        for f in tqdm(files, desc=\" Extracting MFCC features\"):\n            if f.lower().endswith((\".wav\", \".mp3\", \".flac\")):\n                path = os.path.join(root, f)\n                features = extract_mfcc_features(path)\n                if features is not None:\n                    records.append([f, *features])\n\n    # Create DataFrame with columns mfcc_1 ... mfcc_40\n    col_names = [\"file_name\"] + [f\"mfcc_{i+1}\" for i in range(40)]\n    df = pd.DataFrame(records, columns=col_names)\n    return df\n\n# -----------------------------\n# 2  Merge with Labels\n# -----------------------------\ndef merge_with_labels(mfcc_df, labels_df, file_col=\"file_name\", label_col=\"label\"):\n    # Ensure consistent filename format\n    mfcc_df[file_col] = mfcc_df[file_col].apply(lambda x: os.path.basename(x))\n    labels_df[file_col] = labels_df[file_col].apply(lambda x: os.path.basename(x))\n\n    merged_df = pd.merge(mfcc_df, labels_df[[file_col, label_col]], on=file_col, how=\"inner\")\n    print(f\" Merged dataset shape: {merged_df.shape}\")\n    return merged_df\n\n# -----------------------------\n# 3  Model Training (GPU)\n# -----------------------------\ndef train_xgboost_model(df, label_col=\"label\"):\n    if df.empty:\n        raise ValueError(\" DataFrame is empty.\")\n\n    X = df.drop([label_col, \"file_name\"], axis=1)\n    y = df[label_col]\n\n    le = LabelEncoder()\n    y_encoded = le.fit_transform(y)\n\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)\n\n    model = XGBClassifier(\n        n_estimators=300,\n        learning_rate=0.05,\n        max_depth=8,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        tree_method=\"gpu_hist\",  #  GPU enabled\n        use_label_encoder=False,\n        eval_metric=\"mlogloss\"\n    )\n\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n\n    print(\" Model Training Complete!\")\n    print(classification_report(y_test, preds, target_names=le.classes_))\n\n    return model, le, scaler\n\n# -----------------------------\n# 4  Run Entire Pipeline\n# -----------------------------\nif __name__ == \"__main__\":\n    # Path to audio folder (e.g. /content/drive/MyDrive/EDA_DPA/Calls_Audio/911_vocals)\n    AUDIO_DIR = \"/kaggle/input/911-recordings/911_recordings\"\n\n    # Path to your existing label dataframe (must have columns: file_name, label)\n    LABELS_CSV = \"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\"\n    labels_df = pd.read_csv(LABELS_CSV)\n\n    # Step 1: Extract MFCCs\n    mfcc_df = build_mfcc_dataset(AUDIO_DIR)\n    print(f\" Extracted MFCC DataFrame shape: {mfcc_df.shape}\")\n    mfcc_df.to_csv(\"mfcc_features.csv\", index=False)\n\n    # Step 2: Merge with labels\n    final_df = merge_with_labels(mfcc_df, labels_df)\n    final_df.to_csv(\"merged_mfcc_dataset.csv\", index=False)\n\n    # Step 3: Train model\n    model, le, scaler = train_xgboost_model(final_df)\n\n    print(\" Training pipeline complete! Model ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T03:51:43.676540Z","iopub.execute_input":"2025-10-08T03:51:43.676814Z","iopub.status.idle":"2025-10-08T04:08:45.308670Z","shell.execute_reply.started":"2025-10-08T03:51:43.676793Z","shell.execute_reply":"2025-10-08T04:08:45.307803Z"}},"outputs":[{"name":"stderr","text":" Extracting MFCC features:   4%|         | 27/708 [00:38<10:31,  1.08it/s][src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n Extracting MFCC features:   5%|         | 33/708 [00:48<17:57,  1.60s/it]Note: Illegal Audio-MPEG-Header 0x7374616e at offset 11139840.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\n Extracting MFCC features:   6%|         | 42/708 [00:57<09:54,  1.12it/s]Note: Illegal Audio-MPEG-Header 0x7374616e at offset 11139840.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\n Extracting MFCC features:   7%|         | 52/708 [01:14<15:46,  1.44s/it]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:   9%|         | 65/708 [01:28<10:54,  1.02s/it][src/libmpg123/id3.c:INT123_parse_new_id3():950] warning: ID3v2: unrealistic small tag lengh 0, skipping\n Extracting MFCC features:  11%|         | 75/708 [01:35<06:43,  1.57it/s]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  13%|        | 89/708 [01:51<08:39,  1.19it/s][src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n[src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n[src/libmpg123/layer3.c:INT123_do_layer3():1844] error: dequantization failed!\n Extracting MFCC features:  14%|        | 98/708 [02:06<12:05,  1.19s/it]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  15%|        | 106/708 [02:16<10:52,  1.08s/it]Note: Illegal Audio-MPEG-Header 0x7374616e at offset 852480.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\n Extracting MFCC features:  18%|        | 126/708 [02:34<06:22,  1.52it/s][src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n Extracting MFCC features:  19%|        | 135/708 [02:43<09:54,  1.04s/it]/tmp/ipykernel_36/3400736661.py:16: UserWarning: PySoundFile failed. Trying audioread instead.\n  y, sr = librosa.load(file_path, sr=sr)\n/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n\tDeprecated as of librosa version 0.10.0.\n\tIt will be removed in librosa version 1.0.\n  y, sr_native = __audioread_load(path, offset, duration, dtype)\n[src/libmpg123/parse.c:skip_junk():1260] error: Giving up searching valid MPEG header after 65536 bytes of junk.\n Extracting MFCC features:  19%|        | 137/708 [02:46<11:27,  1.20s/it]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  23%|       | 162/708 [03:24<08:53,  1.02it/s]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  23%|       | 165/708 [03:26<06:21,  1.42it/s]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  26%|       | 181/708 [04:01<15:58,  1.82s/it]/tmp/ipykernel_36/3400736661.py:16: UserWarning: PySoundFile failed. Trying audioread instead.\n  y, sr = librosa.load(file_path, sr=sr)\n/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n\tDeprecated as of librosa version 0.10.0.\n\tIt will be removed in librosa version 1.0.\n  y, sr_native = __audioread_load(path, offset, duration, dtype)\nNote: Illegal Audio-MPEG-Header 0x00000000 at offset 5219.\nNote: Trying to resync...\nNote: Hit end of (available) data during resync.\n Extracting MFCC features:  26%|       | 182/708 [04:01<11:27,  1.31s/it]","output_type":"stream"},{"name":"stdout","text":" Error processing /kaggle/input/911-recordings/911_recordings/call_506.mp3: \n","output_type":"stream"},{"name":"stderr","text":" Extracting MFCC features:  26%|       | 186/708 [04:05<08:59,  1.03s/it][src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n Extracting MFCC features:  31%|      | 222/708 [04:50<10:28,  1.29s/it]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  34%|      | 244/708 [05:23<12:32,  1.62s/it]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  42%|     | 294/708 [06:41<11:06,  1.61s/it]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  46%|     | 325/708 [07:39<04:51,  1.32it/s]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  47%|     | 333/708 [07:51<10:09,  1.62s/it][src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n Extracting MFCC features:  49%|     | 348/708 [08:14<05:54,  1.02it/s][src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n Extracting MFCC features:  56%|    | 395/708 [09:28<06:32,  1.25s/it]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  56%|    | 398/708 [09:32<06:21,  1.23s/it][src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n Extracting MFCC features:  64%|   | 456/708 [10:48<06:11,  1.47s/it][src/libmpg123/id3.c:INT123_parse_new_id3():950] warning: ID3v2: unrealistic small tag lengh 0, skipping\n Extracting MFCC features:  66%|   | 468/708 [11:05<04:15,  1.07s/it]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  66%|   | 469/708 [11:05<03:20,  1.19it/s]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  67%|   | 471/708 [11:08<05:19,  1.35s/it][src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n Extracting MFCC features:  76%|  | 540/708 [13:11<03:32,  1.27s/it]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  82%| | 582/708 [14:08<02:44,  1.31s/it][src/libmpg123/id3.c:process_extra():684] error: No extra frame text / valid description?\n Extracting MFCC features:  83%| | 589/708 [14:18<02:16,  1.15s/it]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  84%| | 593/708 [14:28<02:51,  1.49s/it][src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n Extracting MFCC features:  84%| | 598/708 [14:33<01:49,  1.00it/s]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  90%| | 637/708 [15:15<01:00,  1.17it/s][src/libmpg123/id3.c:process_extra():684] error: No extra frame text / valid description?\n[src/libmpg123/id3.c:process_extra():684] error: No extra frame text / valid description?\n Extracting MFCC features:  91%| | 641/708 [15:18<00:52,  1.26it/s][src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n Extracting MFCC features:  95%|| 672/708 [16:08<00:41,  1.16s/it][src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n Extracting MFCC features:  95%|| 676/708 [16:11<00:25,  1.26it/s]Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n Extracting MFCC features:  97%|| 690/708 [16:32<00:18,  1.04s/it][src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n Extracting MFCC features:  99%|| 701/708 [16:53<00:09,  1.38s/it][src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n Extracting MFCC features: 100%|| 708/708 [17:00<00:00,  1.44s/it]","output_type":"stream"},{"name":"stdout","text":" Extracted MFCC DataFrame shape: (706, 41)\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'file_name'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/3400736661.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;31m# Step 2: Merge with labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_with_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmfcc_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m     \u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"merged_mfcc_dataset.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/3400736661.py\u001b[0m in \u001b[0;36mmerge_with_labels\u001b[0;34m(mfcc_df, labels_df, file_col, label_col)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Ensure consistent filename format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mmfcc_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmfcc_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mlabels_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_col\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mmerged_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmfcc_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_col\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_col\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"inner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'file_name'"],"ename":"KeyError","evalue":"'file_name'","output_type":"error"}],"execution_count":9},{"cell_type":"code","source":"#  Check what columns exist\nprint(\"MFCC DF Columns:\", mfcc_df.columns.tolist())\nprint(\"Labels DF Columns:\", labels_df.columns.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:09:44.296057Z","iopub.execute_input":"2025-10-08T04:09:44.296345Z","iopub.status.idle":"2025-10-08T04:09:44.300847Z","shell.execute_reply.started":"2025-10-08T04:09:44.296325Z","shell.execute_reply":"2025-10-08T04:09:44.300290Z"}},"outputs":[{"name":"stdout","text":"MFCC DF Columns: ['file_name', 'mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4', 'mfcc_5', 'mfcc_6', 'mfcc_7', 'mfcc_8', 'mfcc_9', 'mfcc_10', 'mfcc_11', 'mfcc_12', 'mfcc_13', 'mfcc_14', 'mfcc_15', 'mfcc_16', 'mfcc_17', 'mfcc_18', 'mfcc_19', 'mfcc_20', 'mfcc_21', 'mfcc_22', 'mfcc_23', 'mfcc_24', 'mfcc_25', 'mfcc_26', 'mfcc_27', 'mfcc_28', 'mfcc_29', 'mfcc_30', 'mfcc_31', 'mfcc_32', 'mfcc_33', 'mfcc_34', 'mfcc_35', 'mfcc_36', 'mfcc_37', 'mfcc_38', 'mfcc_39', 'mfcc_40']\nLabels DF Columns: ['File Name', 'Transcript', 'Label']\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport xgboost as xgb\nimport joblib\nimport os\n\n# -------------------------------\n#  Step 1: Load feature and label data\n# -------------------------------\nmfcc_path = \"/kaggle/working/mfcc_features.csv\"        # Your MFCC file\nlabels_path = \"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\"              # Your labels file\n\ndf_features = pd.read_csv(mfcc_path)\ndf_labels = pd.read_csv(labels_path)\n\nprint(\"MFCC DF Columns:\", df_features.columns.tolist())\nprint(\"Labels DF Columns:\", df_labels.columns.tolist())\n\n# -------------------------------\n#  Step 2: Normalize column names for merging\n# -------------------------------\ndf_features['file_name'] = df_features['file_name'].str.strip().str.lower()\ndf_labels['File Name'] = df_labels['File Name'].str.strip().str.lower()\n\n# -------------------------------\n#  Step 3: Merge features with labels\n# -------------------------------\ndf = pd.merge(df_features, df_labels, left_on='file_name', right_on='File Name')\n\nif df.empty:\n    raise ValueError(\" Merge failed  filenames may not match. Check file name formats!\")\n\nprint(f\" Merged Data  Shape: {df.shape}\")\nprint(df.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:11:45.962561Z","iopub.execute_input":"2025-10-08T04:11:45.963168Z","iopub.status.idle":"2025-10-08T04:11:46.049888Z","shell.execute_reply.started":"2025-10-08T04:11:45.963145Z","shell.execute_reply":"2025-10-08T04:11:46.049295Z"}},"outputs":[{"name":"stdout","text":"MFCC DF Columns: ['file_name', 'mfcc_1', 'mfcc_2', 'mfcc_3', 'mfcc_4', 'mfcc_5', 'mfcc_6', 'mfcc_7', 'mfcc_8', 'mfcc_9', 'mfcc_10', 'mfcc_11', 'mfcc_12', 'mfcc_13', 'mfcc_14', 'mfcc_15', 'mfcc_16', 'mfcc_17', 'mfcc_18', 'mfcc_19', 'mfcc_20', 'mfcc_21', 'mfcc_22', 'mfcc_23', 'mfcc_24', 'mfcc_25', 'mfcc_26', 'mfcc_27', 'mfcc_28', 'mfcc_29', 'mfcc_30', 'mfcc_31', 'mfcc_32', 'mfcc_33', 'mfcc_34', 'mfcc_35', 'mfcc_36', 'mfcc_37', 'mfcc_38', 'mfcc_39', 'mfcc_40']\nLabels DF Columns: ['File Name', 'Transcript', 'Label']\n Merged Data  Shape: (705, 44)\n      file_name     mfcc_1     mfcc_2     mfcc_3     mfcc_4     mfcc_5  \\\n0   call_71.mp3 -329.07617  196.85385 -62.192670 -25.195879  37.234642   \n1  call_456.mp3 -294.39798  145.84119 -37.339542  -0.656235  -2.593803   \n2  call_347.mp3 -199.22401  166.96788 -73.233670   0.358136  22.468584   \n3  call_664.mp3 -322.22424  162.61769 -33.725906  37.643116  11.074237   \n4  call_208.mp3 -316.49524  196.31496 -52.954403 -16.815434  23.800753   \n\n      mfcc_6     mfcc_7     mfcc_8     mfcc_9  ...   mfcc_34   mfcc_35  \\\n0 -29.520422 -36.170720   8.375070 -14.953630  ... -5.336048 -7.460006   \n1  -1.523595 -19.012043  -5.373490 -11.148335  ... -1.271754  0.875085   \n2 -38.144577  -4.105538   1.665002 -20.147820  ...  2.168981 -1.683257   \n3 -11.417717  -1.079967 -21.978110  -3.201914  ... -0.719344  0.899284   \n4 -22.488424 -10.840758  -2.665913 -23.216673  ... -0.097112  1.372172   \n\n    mfcc_36   mfcc_37   mfcc_38   mfcc_39   mfcc_40     File Name  \\\n0 -1.322120 -2.883095 -6.308777 -1.840010 -2.623923   call_71.mp3   \n1  0.436271  0.104171  1.844694 -2.130319  2.336743  call_456.mp3   \n2  3.926534  0.975193 -1.751384  2.117357 -0.560500  call_347.mp3   \n3  0.589440  0.372172  0.349568 -1.581300  1.166973  call_664.mp3   \n4  1.852317  0.878914  1.457996  1.800440  1.312121  call_208.mp3   \n\n                                          Transcript     Label  \n0  This recording was prepared by 911 dispatch co...  positive  \n1  I'm calling the Santa Fe 911. What is the loca...  positive  \n2  Nine one one. What is the address of your emer...  positive  \n3  Nine one one. Where's your emergency? I need a...  positive  \n4  Information. 9, 1, 1. What's the address of th...  negative  \n\n[5 rows x 44 columns]\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:12:03.144350Z","iopub.execute_input":"2025-10-08T04:12:03.145037Z","iopub.status.idle":"2025-10-08T04:12:03.167912Z","shell.execute_reply.started":"2025-10-08T04:12:03.145013Z","shell.execute_reply":"2025-10-08T04:12:03.167297Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"      file_name     mfcc_1     mfcc_2     mfcc_3     mfcc_4     mfcc_5  \\\n0   call_71.mp3 -329.07617  196.85385 -62.192670 -25.195879  37.234642   \n1  call_456.mp3 -294.39798  145.84119 -37.339542  -0.656235  -2.593803   \n2  call_347.mp3 -199.22401  166.96788 -73.233670   0.358136  22.468584   \n3  call_664.mp3 -322.22424  162.61769 -33.725906  37.643116  11.074237   \n4  call_208.mp3 -316.49524  196.31496 -52.954403 -16.815434  23.800753   \n\n      mfcc_6     mfcc_7     mfcc_8     mfcc_9  ...   mfcc_34   mfcc_35  \\\n0 -29.520422 -36.170720   8.375070 -14.953630  ... -5.336048 -7.460006   \n1  -1.523595 -19.012043  -5.373490 -11.148335  ... -1.271754  0.875085   \n2 -38.144577  -4.105538   1.665002 -20.147820  ...  2.168981 -1.683257   \n3 -11.417717  -1.079967 -21.978110  -3.201914  ... -0.719344  0.899284   \n4 -22.488424 -10.840758  -2.665913 -23.216673  ... -0.097112  1.372172   \n\n    mfcc_36   mfcc_37   mfcc_38   mfcc_39   mfcc_40     File Name  \\\n0 -1.322120 -2.883095 -6.308777 -1.840010 -2.623923   call_71.mp3   \n1  0.436271  0.104171  1.844694 -2.130319  2.336743  call_456.mp3   \n2  3.926534  0.975193 -1.751384  2.117357 -0.560500  call_347.mp3   \n3  0.589440  0.372172  0.349568 -1.581300  1.166973  call_664.mp3   \n4  1.852317  0.878914  1.457996  1.800440  1.312121  call_208.mp3   \n\n                                          Transcript     Label  \n0  This recording was prepared by 911 dispatch co...  positive  \n1  I'm calling the Santa Fe 911. What is the loca...  positive  \n2  Nine one one. What is the address of your emer...  positive  \n3  Nine one one. Where's your emergency? I need a...  positive  \n4  Information. 9, 1, 1. What's the address of th...  negative  \n\n[5 rows x 44 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>mfcc_1</th>\n      <th>mfcc_2</th>\n      <th>mfcc_3</th>\n      <th>mfcc_4</th>\n      <th>mfcc_5</th>\n      <th>mfcc_6</th>\n      <th>mfcc_7</th>\n      <th>mfcc_8</th>\n      <th>mfcc_9</th>\n      <th>...</th>\n      <th>mfcc_34</th>\n      <th>mfcc_35</th>\n      <th>mfcc_36</th>\n      <th>mfcc_37</th>\n      <th>mfcc_38</th>\n      <th>mfcc_39</th>\n      <th>mfcc_40</th>\n      <th>File Name</th>\n      <th>Transcript</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>call_71.mp3</td>\n      <td>-329.07617</td>\n      <td>196.85385</td>\n      <td>-62.192670</td>\n      <td>-25.195879</td>\n      <td>37.234642</td>\n      <td>-29.520422</td>\n      <td>-36.170720</td>\n      <td>8.375070</td>\n      <td>-14.953630</td>\n      <td>...</td>\n      <td>-5.336048</td>\n      <td>-7.460006</td>\n      <td>-1.322120</td>\n      <td>-2.883095</td>\n      <td>-6.308777</td>\n      <td>-1.840010</td>\n      <td>-2.623923</td>\n      <td>call_71.mp3</td>\n      <td>This recording was prepared by 911 dispatch co...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>call_456.mp3</td>\n      <td>-294.39798</td>\n      <td>145.84119</td>\n      <td>-37.339542</td>\n      <td>-0.656235</td>\n      <td>-2.593803</td>\n      <td>-1.523595</td>\n      <td>-19.012043</td>\n      <td>-5.373490</td>\n      <td>-11.148335</td>\n      <td>...</td>\n      <td>-1.271754</td>\n      <td>0.875085</td>\n      <td>0.436271</td>\n      <td>0.104171</td>\n      <td>1.844694</td>\n      <td>-2.130319</td>\n      <td>2.336743</td>\n      <td>call_456.mp3</td>\n      <td>I'm calling the Santa Fe 911. What is the loca...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>call_347.mp3</td>\n      <td>-199.22401</td>\n      <td>166.96788</td>\n      <td>-73.233670</td>\n      <td>0.358136</td>\n      <td>22.468584</td>\n      <td>-38.144577</td>\n      <td>-4.105538</td>\n      <td>1.665002</td>\n      <td>-20.147820</td>\n      <td>...</td>\n      <td>2.168981</td>\n      <td>-1.683257</td>\n      <td>3.926534</td>\n      <td>0.975193</td>\n      <td>-1.751384</td>\n      <td>2.117357</td>\n      <td>-0.560500</td>\n      <td>call_347.mp3</td>\n      <td>Nine one one. What is the address of your emer...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>call_664.mp3</td>\n      <td>-322.22424</td>\n      <td>162.61769</td>\n      <td>-33.725906</td>\n      <td>37.643116</td>\n      <td>11.074237</td>\n      <td>-11.417717</td>\n      <td>-1.079967</td>\n      <td>-21.978110</td>\n      <td>-3.201914</td>\n      <td>...</td>\n      <td>-0.719344</td>\n      <td>0.899284</td>\n      <td>0.589440</td>\n      <td>0.372172</td>\n      <td>0.349568</td>\n      <td>-1.581300</td>\n      <td>1.166973</td>\n      <td>call_664.mp3</td>\n      <td>Nine one one. Where's your emergency? I need a...</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>call_208.mp3</td>\n      <td>-316.49524</td>\n      <td>196.31496</td>\n      <td>-52.954403</td>\n      <td>-16.815434</td>\n      <td>23.800753</td>\n      <td>-22.488424</td>\n      <td>-10.840758</td>\n      <td>-2.665913</td>\n      <td>-23.216673</td>\n      <td>...</td>\n      <td>-0.097112</td>\n      <td>1.372172</td>\n      <td>1.852317</td>\n      <td>0.878914</td>\n      <td>1.457996</td>\n      <td>1.800440</td>\n      <td>1.312121</td>\n      <td>call_208.mp3</td>\n      <td>Information. 9, 1, 1. What's the address of th...</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows  44 columns</p>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"# Drop Neutral rows\ndf = df[df[\"Label\"].str.lower() != \"neutral\"].reset_index(drop=True)\n\n# Re-prepare X and y\nX = df[[c for c in df.columns if c.startswith(\"mfcc_\")]]\ny = df[\"Label\"]\n\n# Encode labels\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Train-test split\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n)\n\nprint(f\" Data ready. Classes after dropping Neutral: {le.classes_}\")\n\n\n# -------------------------------\n#  Step 6: Train XGBoost (GPU if available)\n# -------------------------------\nuse_gpu = xgb.core._has_cuda_support()\n\nparams = {\n    \"objective\": \"multi:softmax\",\n    \"num_class\": len(np.unique(y_encoded)),\n    \"eval_metric\": \"mlogloss\",\n    \"tree_method\": \"gpu_hist\" if use_gpu else \"hist\",\n    \"predictor\": \"gpu_predictor\" if use_gpu else \"cpu_predictor\",\n    \"max_depth\": 6,\n    \"learning_rate\": 0.1,\n    \"n_estimators\": 200\n}\n\nprint(f\" Training XGBoost Model ({'GPU' if use_gpu else 'CPU'} Mode)...\")\n\nmodel = xgb.XGBClassifier(**params)\nmodel.fit(X_train, y_train)\n\n# -------------------------------\n#  Step 7: Evaluate\n# -------------------------------\ny_pred = model.predict(X_test)\nprint(\"\\n Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=le.classes_))\n\nprint(\"\\n Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# -------------------------------\n#  Step 8: Save model & encoders\n# -------------------------------\nos.makedirs(\"models\", exist_ok=True)\njoblib.dump(model, \"models/xgb_audio_model.pkl\")\njoblib.dump(le, \"models/label_encoder.pkl\")\njoblib.dump(scaler, \"models/scaler.pkl\")\n\nprint(\"\\n Model and preprocessing artifacts saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:13:26.698961Z","iopub.execute_input":"2025-10-08T04:13:26.699550Z","iopub.status.idle":"2025-10-08T04:13:26.722643Z","shell.execute_reply.started":"2025-10-08T04:13:26.699524Z","shell.execute_reply":"2025-10-08T04:13:26.721531Z"}},"outputs":[{"name":"stdout","text":" Data ready. Classes after dropping Neutral: ['negative' 'positive']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/2205550325.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#  Step 6: Train XGBoost (GPU if available)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# -------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0muse_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_cuda_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m params = {\n","\u001b[0;31mAttributeError\u001b[0m: module 'xgboost.core' has no attribute '_has_cuda_support'"],"ename":"AttributeError","evalue":"module 'xgboost.core' has no attribute '_has_cuda_support'","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport joblib\nimport os\n\n# -------------------------------\n# Step 6: Train Random Forest\n# -------------------------------\nrf_model = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=None,\n    random_state=42,\n    n_jobs=-1       # uses all cores\n)\nrf_model.fit(X_train, y_train)\n\n# -------------------------------\n# Step 7: Evaluate\n# -------------------------------\ny_pred = rf_model.predict(X_test)\n\nprint(\"\\n Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=le.classes_))\n\nprint(\"\\n Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# -------------------------------\n# Step 8: Save model & encoders\n# -------------------------------\nos.makedirs(\"models\", exist_ok=True)\njoblib.dump(rf_model, \"models/rf_audio_model.pkl\")\njoblib.dump(le, \"models/label_encoder.pkl\")\njoblib.dump(scaler, \"models/scaler.pkl\")\n\nprint(\"\\n Random Forest model and preprocessing artifacts saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:14:43.239254Z","iopub.execute_input":"2025-10-08T04:14:43.239562Z","iopub.status.idle":"2025-10-08T04:14:44.376680Z","shell.execute_reply.started":"2025-10-08T04:14:43.239539Z","shell.execute_reply":"2025-10-08T04:14:44.375946Z"}},"outputs":[{"name":"stdout","text":"\n Classification Report:\n              precision    recall  f1-score   support\n\n    negative       1.00      0.05      0.10        20\n    positive       0.86      1.00      0.93       121\n\n    accuracy                           0.87       141\n   macro avg       0.93      0.53      0.51       141\nweighted avg       0.88      0.87      0.81       141\n\n\n Confusion Matrix:\n[[  1  19]\n [  0 121]]\n\n Random Forest model and preprocessing artifacts saved successfully.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport joblib\nimport os\n\n# -------------------------------\n# Step 1: Load MFCC features and labels\n# -------------------------------\nmfcc_path = \"/kaggle/working/mfcc_features.csv\"    # your MFCC features\nlabels_path = \"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\"         # your labels\n\ndf_features = pd.read_csv(mfcc_path)\ndf_labels = pd.read_csv(labels_path)\n\n# Normalize filenames for merge\ndf_features['file_name'] = df_features['file_name'].str.strip().str.lower()\ndf_labels['File Name'] = df_labels['File Name'].str.strip().str.lower()\n\n# -------------------------------\n# Step 2: Merge features with labels\n# -------------------------------\ndf = pd.merge(df_features, df_labels, left_on='file_name', right_on='File Name')\n\n# -------------------------------\n# Step 3: Drop Neutral class\n# -------------------------------\ndf = df[df[\"Label\"].str.lower() != \"neutral\"].reset_index(drop=True)\n\n# -------------------------------\n# Step 4: Prepare data for training\n# -------------------------------\nX = df[[c for c in df.columns if c.startswith(\"mfcc_\")]]\ny = df[\"Label\"]\n\n# Encode labels\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\n# Scale features (optional for RF)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------\n# Step 5: Train-test split\n# -------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n)\n\n# -------------------------------\n# Step 6: Train Random Forest with class balancing and overfitting control\n# -------------------------------\nrf_model = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=10,            # helps reduce overfitting\n    min_samples_leaf=2,      # helps generalization\n    random_state=42,\n    n_jobs=-1,\n    class_weight=\"balanced\"  # handles class imbalance\n)\nrf_model.fit(X_train, y_train)\n\n# -------------------------------\n# Step 7: Evaluate\n# -------------------------------\ny_pred = rf_model.predict(X_test)\n\nprint(\"\\n Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=le.classes_))\n\nprint(\"\\n Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Cross-validation check\ncv_scores = cross_val_score(rf_model, X_scaled, y_encoded, cv=5, scoring='f1_weighted')\nprint(\"\\n 5-fold CV Weighted F1-score:\", cv_scores.mean())\n\n# -------------------------------\n# Step 8: Save model & encoders\n# -------------------------------\nos.makedirs(\"models\", exist_ok=True)\njoblib.dump(rf_model, \"models/rf_audio_model.pkl\")\njoblib.dump(le, \"models/label_encoder.pkl\")\njoblib.dump(scaler, \"models/scaler.pkl\")\n\nprint(\"\\n Random Forest model and preprocessing artifacts saved successfully.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:16:38.636208Z","iopub.execute_input":"2025-10-08T04:16:38.636535Z","iopub.status.idle":"2025-10-08T04:16:41.893718Z","shell.execute_reply.started":"2025-10-08T04:16:38.636513Z","shell.execute_reply":"2025-10-08T04:16:41.893032Z"}},"outputs":[{"name":"stdout","text":"\n Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.50      0.05      0.09        20\n    positive       0.86      0.99      0.92       121\n\n    accuracy                           0.86       141\n   macro avg       0.68      0.52      0.51       141\nweighted avg       0.81      0.86      0.81       141\n\n\n Confusion Matrix:\n[[  1  19]\n [  1 120]]\n\n 5-fold CV Weighted F1-score: 0.7948552851612709\n\n Random Forest model and preprocessing artifacts saved successfully.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"# -------------------------------\n# Step 0: Undersample to balance classes\n# -------------------------------\n# Find the class with the minimum number of samples\nmin_count = df['Label'].value_counts().min()\n\n# Take min_count samples from each class\ndf_balanced = df.groupby('Label').sample(n=min_count, random_state=42).reset_index(drop=True)\n\nprint(\" Balanced dataset shape:\", df_balanced.shape)\nprint(df_balanced['Label'].value_counts())\n\n# -------------------------------\n# Step 1: Prepare X and y\n# -------------------------------\nX = df_balanced[[c for c in df_balanced.columns if c.startswith(\"mfcc_\")]]\ny = df_balanced[\"Label\"]\n\n# Encode labels\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\n# Scale features (optional)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------\n# Step 2: Train-test split\n# -------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n)\n\n# -------------------------------\n# Step 3: Train Random Forest\n# -------------------------------\nrf_model = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=10,\n    min_samples_leaf=2,\n    random_state=42,\n    n_jobs=-1\n)\nrf_model.fit(X_train, y_train)\n\n# -------------------------------\n# Step 4: Evaluate\n# -------------------------------\ny_pred = rf_model.predict(X_test)\nprint(\"\\n Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=le.classes_))\nprint(\"\\n Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:17:30.411423Z","iopub.execute_input":"2025-10-08T04:17:30.411709Z","iopub.status.idle":"2025-10-08T04:17:30.855234Z","shell.execute_reply.started":"2025-10-08T04:17:30.411689Z","shell.execute_reply":"2025-10-08T04:17:30.854455Z"}},"outputs":[{"name":"stdout","text":" Balanced dataset shape: (200, 44)\nLabel\nnegative    100\npositive    100\nName: count, dtype: int64\n\n Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.70      0.35      0.47        20\n    positive       0.57      0.85      0.68        20\n\n    accuracy                           0.60        40\n   macro avg       0.63      0.60      0.57        40\nweighted avg       0.63      0.60      0.57        40\n\n\n Confusion Matrix:\n[[ 7 13]\n [ 3 17]]\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.decomposition import PCA\nimport joblib\nimport os\n\n# -------------------------------\n# Step 1: Load MFCC features and labels\n# -------------------------------\nmfcc_path = \"/kaggle/working/mfcc_features.csv\"    # your MFCC features CSV\nlabels_path = \"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\"         # your labels CSV\n\ndf_features = pd.read_csv(mfcc_path)\ndf_labels = pd.read_csv(labels_path)\n\n# Normalize filenames for merge\ndf_features['file_name'] = df_features['file_name'].str.strip().str.lower()\ndf_labels['File Name'] = df_labels['File Name'].str.strip().str.lower()\n\n# -------------------------------\n# Step 2: Merge features with labels\n# -------------------------------\ndf = pd.merge(df_features, df_labels, left_on='file_name', right_on='File Name')\n\n# -------------------------------\n# Step 3: Drop Neutral class\n# -------------------------------\ndf = df[df[\"Label\"].str.lower() != \"neutral\"].reset_index(drop=True)\n\n# -------------------------------\n# Step 4: Balance dataset by undersampling\n# -------------------------------\nmin_count = df['Label'].value_counts().min()\ndf_balanced = df.groupby('Label').sample(n=min_count, random_state=42).reset_index(drop=True)\nprint(\" Balanced dataset shape:\", df_balanced.shape)\nprint(df_balanced['Label'].value_counts())\n\n# -------------------------------\n# Step 5: Prepare X and y\n# -------------------------------\nX = df_balanced[[c for c in df_balanced.columns if c.startswith(\"mfcc_\")]]\ny = df_balanced[\"Label\"]\n\n# Encode labels\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------\n# Step 6: Dimensionality reduction with PCA\n# -------------------------------\npca = PCA(n_components=15, random_state=42)\nX_reduced = pca.fit_transform(X_scaled)\n\n# -------------------------------\n# Step 7: Train-test split\n# -------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X_reduced, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n)\n\n# -------------------------------\n# Step 8: Train Random Forest\n# -------------------------------\nrf_model = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=7,            # control overfitting\n    min_samples_leaf=2,     # control overfitting\n    random_state=42,\n    n_jobs=-1\n)\nrf_model.fit(X_train, y_train)\n\n# -------------------------------\n# Step 9: Evaluate\n# -------------------------------\ny_pred = rf_model.predict(X_test)\n\nprint(\"\\n Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=le.classes_))\n\nprint(\"\\n Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Cross-validation to check stability\ncv_scores = cross_val_score(rf_model, X_reduced, y_encoded, cv=5, scoring='f1_weighted')\nprint(\"\\n 5-fold CV Weighted F1-score:\", cv_scores.mean())\n\n# -------------------------------\n# Step 10: Save model and preprocessing artifacts\n# -------------------------------\nos.makedirs(\"models\", exist_ok=True)\njoblib.dump(rf_model, \"models/rf_audio_model.pkl\")\njoblib.dump(le, \"models/label_encoder.pkl\")\njoblib.dump(scaler, \"models/scaler.pkl\")\njoblib.dump(pca, \"models/pca_transformer.pkl\")\n\nprint(\"\\n Random Forest model and preprocessing artifacts saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:19:02.978487Z","iopub.execute_input":"2025-10-08T04:19:02.979165Z","iopub.status.idle":"2025-10-08T04:19:05.660182Z","shell.execute_reply.started":"2025-10-08T04:19:02.979139Z","shell.execute_reply":"2025-10-08T04:19:05.659534Z"}},"outputs":[{"name":"stdout","text":" Balanced dataset shape: (200, 44)\nLabel\nnegative    100\npositive    100\nName: count, dtype: int64\n\n Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.67      0.50      0.57        20\n    positive       0.60      0.75      0.67        20\n\n    accuracy                           0.62        40\n   macro avg       0.63      0.62      0.62        40\nweighted avg       0.63      0.62      0.62        40\n\n\n Confusion Matrix:\n[[10 10]\n [ 5 15]]\n\n 5-fold CV Weighted F1-score: 0.5527994608740686\n\n Random Forest model and preprocessing artifacts saved successfully.\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.decomposition import PCA\nimport joblib\nimport os\n\n# -------------------------------\n# Step 1: Load MFCC features and labels\n# -------------------------------\nmfcc_path = \"/kaggle/working/mfcc_features.csv\"    # your MFCC features CSV\nlabels_path = \"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\"         # your labels CSV\n\ndf_features = pd.read_csv(mfcc_path)\ndf_labels = pd.read_csv(labels_path)\n\n# Normalize filenames for merge\ndf_features['file_name'] = df_features['file_name'].str.strip().str.lower()\ndf_labels['File Name'] = df_labels['File Name'].str.strip().str.lower()\n\n# -------------------------------\n# Step 2: Merge features with labels\n# -------------------------------\ndf = pd.merge(df_features, df_labels, left_on='file_name', right_on='File Name')\n\n# -------------------------------\n# Step 3: Drop Neutral class\n# -------------------------------\ndf = df[df[\"Label\"].str.lower() != \"neutral\"].reset_index(drop=True)\n\n# -------------------------------\n# Step 4: Balance dataset by undersampling\n# -------------------------------\nmin_count = df['Label'].value_counts().min()\ndf_balanced = df.groupby('Label').sample(n=min_count, random_state=42).reset_index(drop=True)\nprint(\" Balanced dataset shape:\", df_balanced.shape)\nprint(df_balanced['Label'].value_counts())\n\n# -------------------------------\n# Step 5: Prepare X and y\n# -------------------------------\nX = df_balanced[[c for c in df_balanced.columns if c.startswith(\"mfcc_\")]]\ny = df_balanced[\"Label\"]\n\n# Encode labels\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------\n# Step 6: Dimensionality reduction with PCA\n# -------------------------------\npca = PCA(n_components=15, random_state=42)\nX_reduced = pca.fit_transform(X_scaled)\n\n# -------------------------------\n# Step 7: Train-test split\n# -------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X_reduced, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n)\n\n# -------------------------------\n# Step 8: Train Random Forest\n# -------------------------------\nrf_model = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=7,            # control overfitting\n    min_samples_leaf=2,     # control overfitting\n    random_state=42,\n    n_jobs=-1\n)\nrf_model.fit(X_train, y_train)\n\n# -------------------------------\n# Step 9: Evaluate\n# -------------------------------\ny_pred = rf_model.predict(X_test)\n\nprint(\"\\n Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=le.classes_))\n\nprint(\"\\n Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Cross-validation to check stability\ncv_scores = cross_val_score(rf_model, X_reduced, y_encoded, cv=5, scoring='f1_weighted')\nprint(\"\\n 5-fold CV Weighted F1-score:\", cv_scores.mean())\n\n# -------------------------------\n# Step 10: Save model and preprocessing artifacts\n# -------------------------------\nos.makedirs(\"models\", exist_ok=True)\njoblib.dump(rf_model, \"models/rf_audio_model.pkl\")\njoblib.dump(le, \"models/label_encoder.pkl\")\njoblib.dump(scaler, \"models/scaler.pkl\")\njoblib.dump(pca, \"models/pca_transformer.pkl\")\n\nprint(\"\\n Random Forest model and preprocessing artifacts saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:20:29.225339Z","iopub.execute_input":"2025-10-08T04:20:29.225807Z","iopub.status.idle":"2025-10-08T04:20:31.885067Z","shell.execute_reply.started":"2025-10-08T04:20:29.225784Z","shell.execute_reply":"2025-10-08T04:20:31.884442Z"}},"outputs":[{"name":"stdout","text":" Balanced dataset shape: (200, 44)\nLabel\nnegative    100\npositive    100\nName: count, dtype: int64\n\n Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.67      0.50      0.57        20\n    positive       0.60      0.75      0.67        20\n\n    accuracy                           0.62        40\n   macro avg       0.63      0.62      0.62        40\nweighted avg       0.63      0.62      0.62        40\n\n\n Confusion Matrix:\n[[10 10]\n [ 5 15]]\n\n 5-fold CV Weighted F1-score: 0.5527994608740686\n\n Random Forest model and preprocessing artifacts saved successfully.\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"mfcc_path = \"/kaggle/input/mfcc-features/mfcc_features.csv\"    # MFCC features CSV\nlabels_path = \"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\"         # Labels CSV\n\ndf_features = pd.read_csv(mfcc_path)\ndf_labels = pd.read_csv(labels_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:26:54.246020Z","iopub.execute_input":"2025-10-08T04:26:54.246298Z","iopub.status.idle":"2025-10-08T04:26:54.299776Z","shell.execute_reply.started":"2025-10-08T04:26:54.246277Z","shell.execute_reply":"2025-10-08T04:26:54.299224Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"df_features.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:27:01.463182Z","iopub.execute_input":"2025-10-08T04:27:01.463464Z","iopub.status.idle":"2025-10-08T04:27:01.468776Z","shell.execute_reply.started":"2025-10-08T04:27:01.463443Z","shell.execute_reply":"2025-10-08T04:27:01.468119Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(706, 41)"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df_labels.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:27:13.482813Z","iopub.execute_input":"2025-10-08T04:27:13.483534Z","iopub.status.idle":"2025-10-08T04:27:13.488536Z","shell.execute_reply.started":"2025-10-08T04:27:13.483507Z","shell.execute_reply":"2025-10-08T04:27:13.487823Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(706, 3)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"df_features['file_name'] = df_features['file_name'].str.strip().str.lower()\ndf_labels['File Name'] = df_labels['File Name'].str.strip().str.lower()\ndf = pd.merge(df_features, df_labels, left_on='file_name', right_on='File Name')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:27:47.811161Z","iopub.execute_input":"2025-10-08T04:27:47.811755Z","iopub.status.idle":"2025-10-08T04:27:47.821794Z","shell.execute_reply.started":"2025-10-08T04:27:47.811730Z","shell.execute_reply":"2025-10-08T04:27:47.820957Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:27:56.797502Z","iopub.execute_input":"2025-10-08T04:27:56.797769Z","iopub.status.idle":"2025-10-08T04:27:56.802505Z","shell.execute_reply.started":"2025-10-08T04:27:56.797749Z","shell.execute_reply":"2025-10-08T04:27:56.801755Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(705, 44)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"df['Label'].value_counts()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:28:16.364966Z","iopub.execute_input":"2025-10-08T04:28:16.365271Z","iopub.status.idle":"2025-10-08T04:28:16.377879Z","shell.execute_reply.started":"2025-10-08T04:28:16.365249Z","shell.execute_reply":"2025-10-08T04:28:16.377250Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"Label\npositive    604\nnegative    100\nneutral       1\nName: count, dtype: int64"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.decomposition import PCA\nimport joblib\nimport os\n\n# -------------------------------\n# Step 1: Load MFCC features and labels\n# -------------------------------\nmfcc_path = \"/kaggle/input/mfcc-features/mfcc_features.csv\"    # MFCC features CSV\nlabels_path = \"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\"         # Labels CSV\n\ndf_features = pd.read_csv(mfcc_path)\ndf_labels = pd.read_csv(labels_path)\n\n# Normalize filenames for merge\ndf_features['file_name'] = df_features['file_name'].str.strip().str.lower()\ndf_labels['File Name'] = df_labels['File Name'].str.strip().str.lower()\n\n# -------------------------------\n# Step 2: Merge features with labels\n# -------------------------------\ndf = pd.merge(df_features, df_labels, left_on='file_name', right_on='File Name')\n\n# -------------------------------\n# Step 3: Drop Neutral class\n# -------------------------------\ndf = df[df[\"Label\"].str.lower() != \"neutral\"].reset_index(drop=True)\n\n# -------------------------------\n# Step 4: Manual balancing\n# -------------------------------\nneg_df = df[df[\"Label\"].str.lower() == \"negative\"].sample(100, random_state=42)\npos_df = df[df[\"Label\"].str.lower() == \"positive\"].sample(80, random_state=42)\n\nbalanced_df = pd.concat([neg_df, pos_df]).reset_index(drop=True)\nprint(\" Balanced dataset shape:\", balanced_df.shape)\nprint(\"Label counts:\\n\", balanced_df['Label'].value_counts())\n\n# -------------------------------\n# Step 5: Prepare X and y\n# -------------------------------\nX = balanced_df[[c for c in balanced_df.columns if c.startswith(\"mfcc_\")]]\ny = balanced_df[\"Label\"]\n\n# Encode labels\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)\n\n# Scale features\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# -------------------------------\n# Step 6: Dimensionality reduction with PCA\n# -------------------------------\npca = PCA(n_components=15, random_state=42)\nX_reduced = pca.fit_transform(X_scaled)\n\n# -------------------------------\n# Step 7: Train-test split\n# -------------------------------\nX_train, X_test, y_train, y_test = train_test_split(\n    X_reduced, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n)\n\n# -------------------------------\n# Step 8: Train Random Forest\n# -------------------------------\nrf_model = RandomForestClassifier(\n    n_estimators=200,\n    max_depth=7,\n    min_samples_leaf=2,\n    random_state=42,\n    n_jobs=-1\n)\nrf_model.fit(X_train, y_train)\n\n# -------------------------------\n# Step 9: Evaluate\n# -------------------------------\ny_pred = rf_model.predict(X_test)\n\nprint(\"\\n Classification Report:\")\nprint(classification_report(y_test, y_pred, target_names=le.classes_))\n\nprint(\"\\n Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Cross-validation to check stability\ncv_scores = cross_val_score(rf_model, X_reduced, y_encoded, cv=5, scoring='f1_weighted')\nprint(\"\\n 5-fold CV Weighted F1-score:\", cv_scores.mean())\n\n# -------------------------------\n# Step 10: Save model and preprocessing artifacts\n# -------------------------------\nos.makedirs(\"models\", exist_ok=True)\njoblib.dump(rf_model, \"models/rf_audio_model.pkl\")\njoblib.dump(le, \"models/label_encoder.pkl\")\njoblib.dump(scaler, \"models/scaler.pkl\")\njoblib.dump(pca, \"models/pca_transformer.pkl\")\n\nprint(\"\\n Random Forest model and preprocessing artifacts saved successfully.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:29:38.414076Z","iopub.execute_input":"2025-10-08T04:29:38.414800Z","iopub.status.idle":"2025-10-08T04:29:41.075359Z","shell.execute_reply.started":"2025-10-08T04:29:38.414769Z","shell.execute_reply":"2025-10-08T04:29:41.074757Z"}},"outputs":[{"name":"stdout","text":" Balanced dataset shape: (180, 44)\nLabel counts:\n Label\nnegative    100\npositive     80\nName: count, dtype: int64\n\n Classification Report:\n              precision    recall  f1-score   support\n\n    negative       0.62      0.65      0.63        20\n    positive       0.53      0.50      0.52        16\n\n    accuracy                           0.58        36\n   macro avg       0.58      0.57      0.58        36\nweighted avg       0.58      0.58      0.58        36\n\n\n Confusion Matrix:\n[[13  7]\n [ 8  8]]\n\n 5-fold CV Weighted F1-score: 0.5382930828341456\n\n Random Forest model and preprocessing artifacts saved successfully.\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FINAL","metadata":{}},{"cell_type":"code","source":"# ======================\n# 1. Import Libraries\n# ======================\nimport os\nimport numpy as np\nimport pandas as pd\nimport librosa\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import classification_report\n\n# ======================\n# 2. GPU Configuration\n# ======================\nimport tensorflow as tf\ngpus = tf.config.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n        print(f\" Using GPU: {gpus[0].name}\")\n        print(f\"CUDA Detected: {tf.test.is_built_with_cuda()}\")\n    except RuntimeError as e:\n        print(e)\nelse:\n    print(\" No GPU detected. Running on CPU.\")\n\n# ======================\n# 3. Paths and Parameters\n# ======================\nAUDIO_DIR = \"/kaggle/input/911-recordings/911_recordings\"\nCSV_PATH = \"/kaggle/input/labelledtranscriptions/911_calls_with_labels.csv\"\nSAMPLE_RATE = 22050\nDURATION = 5\nSAMPLES_PER_TRACK = SAMPLE_RATE * DURATION\nN_MFCC = 20\n\n# ======================\n# 4. Load Metadata and Balance Dataset\n# ======================\nmetadata = pd.read_csv(CSV_PATH)\n\n# Drop rare/neutral classes\nmetadata = metadata[metadata['Label'].isin(['positive','negative'])]\n\n#metadata['Label'] = metadata['Label'].map({'positive': 'Negative', 'negative': 'Positive'})\n\n# Balance: sample 100 positive, 100 negative\npositive_sample = metadata[metadata['Label']=='positive'].sample(n=90, random_state=42)\nnegative_sample = metadata[metadata['Label']=='negative'].sample(n=100, random_state=42)\nmetadata = pd.concat([positive_sample, negative_sample]).sample(frac=1, random_state=42).reset_index(drop=True)\n\nprint(\"Balanced class counts:\")\nprint(metadata['Label'].value_counts())\n\n# ======================?\n# 5. Feature Extraction\n# ======================\ndef extract_features(file_path):\n    y, sr = librosa.load(file_path, sr=SAMPLE_RATE, duration=DURATION)\n    # Pad/trim\n    if len(y) < SAMPLES_PER_TRACK:\n        y = np.pad(y, (0, SAMPLES_PER_TRACK - len(y)))\n    else:\n        y = y[:SAMPLES_PER_TRACK]\n    \n    # MFCC\n    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=N_MFCC)\n    mfccs_mean = np.mean(mfccs.T, axis=0)\n    \n    # Chroma\n    stft = np.abs(librosa.stft(y))\n    chroma = librosa.feature.chroma_stft(S=stft, sr=sr)\n    chroma_mean = np.mean(chroma.T, axis=0)\n    \n    # Spectral Contrast\n    spec_contrast = librosa.feature.spectral_contrast(S=stft, sr=sr)\n    spec_contrast_mean = np.mean(spec_contrast.T, axis=0)\n    \n    # Combine features\n    features = np.concatenate([mfccs_mean, chroma_mean, spec_contrast_mean])\n    return features\n\n# Process audio files\nX, y_list = [], []\ntotal_files = len(metadata)\nfor i, row in metadata.iterrows():\n    file_path = os.path.join(AUDIO_DIR, row[\"File Name\"])\n    try:\n        features = extract_features(file_path)\n        X.append(features)\n        y_list.append(row[\"Label\"])\n    except Exception as e:\n        print(f\"Error processing {file_path}: {e}\")\n    \n    if (i+1) % 10 == 0 or (i+1) == total_files:\n        print(f\"Processed {i+1}/{total_files} audio files\")\n\nX = np.array(X)\ny_list = np.array(y_list)\nprint(\"Final feature shape:\", X.shape)\n\n# ======================\n# 6. Encode Labels\n# ======================\nencoder = LabelEncoder()\ny_encoded = encoder.fit_transform(y_list)\ny_cat = to_categorical(y_encoded)\nclasses = np.unique(y_encoded)\nprint(\"Classes:\", encoder.classes_)\n\n# ======================\n# 7. Train-Test Split\n# ======================\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y_cat, test_size=0.2, stratify=y_cat, random_state=42\n)\nprint(\"Train/Test shapes:\", X_train.shape, X_test.shape)\n\n# ======================\n# 8. Class Weights\n# ======================\ny_integers = np.argmax(y_train, axis=1)\nclass_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_integers)\nclass_weights_dict = dict(zip(classes, class_weights))\nprint(\"Class weights:\", class_weights_dict)\n\n# ======================\n# 9. Build ANN Model\n# ======================\nmodel = Sequential([\n    Dense(256, activation='relu', input_shape=(X.shape[1],)),\n    Dropout(0.3),\n    Dense(128, activation='relu'),\n    Dropout(0.3),\n    Dense(len(classes), activation='softmax')\n])\n\nmodel.compile(optimizer=Adam(learning_rate=0.001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel.summary()\n\n# ======================\n# 10. Train Model\n# ======================\nwith tf.device('/GPU:0'):\n    history = model.fit(\n        X_train, y_train,\n        validation_split=0.2,\n        epochs=45,\n        batch_size=16,\n        #class_weight=class_weights_dict,\n        verbose=1\n    )\n\n# ======================\n# 11. Evaluate\n# ======================\nwith tf.device('/GPU:0'):\n    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\nprint(f\"\\n Test Accuracy: {test_acc:.4f}\")\n\ny_pred = model.predict(X_test)\ny_pred_classes = np.argmax(y_pred, axis=1)\ny_true = np.argmax(y_test, axis=1)\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_true, y_pred_classes, target_names=encoder.classes_))\n\n# ======================\n# 11b. Confusion Matrix\n# ======================\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncm = confusion_matrix(y_true, y_pred_classes)\nplt.figure(figsize=(6,5))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=encoder.classes_, yticklabels=encoder.classes_)\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.title('Confusion Matrix')\nplt.show()\n\n\n# ======================\n# 12. Save Model\n# ======================\nmodel.save(\"ann_audio_sentiment_model.h5\")\nprint(\" Model saved successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-08T04:51:54.612395Z","iopub.execute_input":"2025-10-08T04:51:54.612986Z","iopub.status.idle":"2025-10-08T04:52:10.886619Z","shell.execute_reply.started":"2025-10-08T04:51:54.612961Z","shell.execute_reply":"2025-10-08T04:52:10.885932Z"}},"outputs":[{"name":"stdout","text":" Using GPU: /physical_device:GPU:0\nCUDA Detected: True\nBalanced class counts:\nLabel\nnegative    100\npositive     90\nName: count, dtype: int64\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/librosa/core/pitch.py:103: UserWarning: Trying to estimate tuning from empty frequency set.\n  return pitch_tuning(\n","output_type":"stream"},{"name":"stdout","text":"Processed 10/190 audio files\nProcessed 20/190 audio files\nProcessed 30/190 audio files\nProcessed 40/190 audio files\n","output_type":"stream"},{"name":"stderr","text":"[src/libmpg123/id3.c:INT123_parse_new_id3():950] warning: ID3v2: unrealistic small tag lengh 0, skipping\nWarning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n","output_type":"stream"},{"name":"stdout","text":"Processed 50/190 audio files\nProcessed 60/190 audio files\nProcessed 70/190 audio files\nProcessed 80/190 audio files\nProcessed 90/190 audio files\n","output_type":"stream"},{"name":"stderr","text":"Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n","output_type":"stream"},{"name":"stdout","text":"Processed 100/190 audio files\n","output_type":"stream"},{"name":"stderr","text":"[src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n","output_type":"stream"},{"name":"stdout","text":"Processed 110/190 audio files\nProcessed 120/190 audio files\nProcessed 130/190 audio files\nProcessed 140/190 audio files\nProcessed 150/190 audio files\nProcessed 160/190 audio files\n","output_type":"stream"},{"name":"stderr","text":"[src/libmpg123/id3.c:process_comment():587] error: No comment text / valid description?\n","output_type":"stream"},{"name":"stdout","text":"Processed 170/190 audio files\n","output_type":"stream"},{"name":"stderr","text":"Warning: Xing stream size off by more than 1%, fuzzy seeking may be even more fuzzy than by design!\n","output_type":"stream"},{"name":"stdout","text":"Processed 180/190 audio files\nProcessed 190/190 audio files\nFinal feature shape: (190, 39)\nClasses: ['negative' 'positive']\nTrain/Test shapes: (152, 39) (38, 39)\nClass weights: {0: 0.95, 1: 1.0555555555555556}\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential_26\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_26\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\n\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n\n dense_78 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                    \u001b[38;5;34m10,240\u001b[0m \n\n dropout_52 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n\n dense_79 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                    \u001b[38;5;34m32,896\u001b[0m \n\n dropout_53 (\u001b[38;5;33mDropout\u001b[0m)             (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                         \u001b[38;5;34m0\u001b[0m \n\n dense_80 (\u001b[38;5;33mDense\u001b[0m)                 (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)                         \u001b[38;5;34m258\u001b[0m \n\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n\n dense_78 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">10,240</span> \n\n dropout_52 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dense_79 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                    <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> \n\n dropout_53 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n\n dense_80 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)                         <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> \n\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m43,394\u001b[0m (169.51 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,394</span> (169.51 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m43,394\u001b[0m (169.51 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">43,394</span> (169.51 KB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 221ms/step - accuracy: 0.5556 - loss: 11.2283 - val_accuracy: 0.3548 - val_loss: 10.3513\nEpoch 2/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4941 - loss: 12.4778 - val_accuracy: 0.6452 - val_loss: 6.6238\nEpoch 3/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5595 - loss: 8.3537 - val_accuracy: 0.6129 - val_loss: 1.8583\nEpoch 4/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5401 - loss: 6.8679 - val_accuracy: 0.6129 - val_loss: 2.0385\nEpoch 5/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5217 - loss: 7.7382 - val_accuracy: 0.5484 - val_loss: 1.4484\nEpoch 6/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4544 - loss: 7.0980 - val_accuracy: 0.5806 - val_loss: 1.8556\nEpoch 7/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6135 - loss: 4.7209 - val_accuracy: 0.6452 - val_loss: 1.5147\nEpoch 8/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.6156 - loss: 3.3553 - val_accuracy: 0.4194 - val_loss: 3.7882\nEpoch 9/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5581 - loss: 5.1957 - val_accuracy: 0.6774 - val_loss: 1.3601\nEpoch 10/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5183 - loss: 5.7675 - val_accuracy: 0.4516 - val_loss: 2.9692\nEpoch 11/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4526 - loss: 4.6721 - val_accuracy: 0.4516 - val_loss: 4.5851\nEpoch 12/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5929 - loss: 3.4628 - val_accuracy: 0.5806 - val_loss: 1.0287\nEpoch 13/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5268 - loss: 4.0595 - val_accuracy: 0.6129 - val_loss: 0.8292\nEpoch 14/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6539 - loss: 2.7974 - val_accuracy: 0.5161 - val_loss: 1.4836\nEpoch 15/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5546 - loss: 3.1114 - val_accuracy: 0.7097 - val_loss: 1.1176\nEpoch 16/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5283 - loss: 2.5198 - val_accuracy: 0.3548 - val_loss: 1.8503\nEpoch 17/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4930 - loss: 2.4022 - val_accuracy: 0.6452 - val_loss: 1.0940\nEpoch 18/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4681 - loss: 2.5257 - val_accuracy: 0.4839 - val_loss: 1.0953\nEpoch 19/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5286 - loss: 1.6398 - val_accuracy: 0.3226 - val_loss: 1.3160\nEpoch 20/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4406 - loss: 2.1793 - val_accuracy: 0.6452 - val_loss: 0.7744\nEpoch 21/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5059 - loss: 2.0446 - val_accuracy: 0.5806 - val_loss: 0.7468\nEpoch 22/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5534 - loss: 1.5729 - val_accuracy: 0.6129 - val_loss: 0.7720\nEpoch 23/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5433 - loss: 1.1839 - val_accuracy: 0.5484 - val_loss: 0.7243\nEpoch 24/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5817 - loss: 1.2021 - val_accuracy: 0.5806 - val_loss: 0.7215\nEpoch 25/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5556 - loss: 1.2261 - val_accuracy: 0.5484 - val_loss: 0.7183\nEpoch 26/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5449 - loss: 1.2480 - val_accuracy: 0.4516 - val_loss: 0.7953\nEpoch 27/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.4889 - loss: 1.1939 - val_accuracy: 0.5484 - val_loss: 0.7890\nEpoch 28/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5311 - loss: 0.9355 - val_accuracy: 0.6129 - val_loss: 0.7471\nEpoch 29/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.5547 - loss: 1.1033 - val_accuracy: 0.3226 - val_loss: 0.7967\nEpoch 30/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5457 - loss: 0.9270 - val_accuracy: 0.5806 - val_loss: 0.7102\nEpoch 31/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5548 - loss: 1.0842 - val_accuracy: 0.4194 - val_loss: 0.7252\nEpoch 32/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5862 - loss: 0.8793 - val_accuracy: 0.5161 - val_loss: 0.6984\nEpoch 33/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.6608 - loss: 0.7971 - val_accuracy: 0.4194 - val_loss: 0.7221\nEpoch 34/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5777 - loss: 0.7898 - val_accuracy: 0.5161 - val_loss: 0.7017\nEpoch 35/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5469 - loss: 0.8361 - val_accuracy: 0.4516 - val_loss: 0.7142\nEpoch 36/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.6331 - loss: 0.7422 - val_accuracy: 0.4839 - val_loss: 0.6914\nEpoch 37/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4770 - loss: 0.8317 - val_accuracy: 0.5806 - val_loss: 0.6881\nEpoch 38/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5561 - loss: 0.7403 - val_accuracy: 0.6129 - val_loss: 0.6753\nEpoch 39/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4937 - loss: 0.8745 - val_accuracy: 0.5806 - val_loss: 0.6734\nEpoch 40/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5859 - loss: 0.7949 - val_accuracy: 0.4839 - val_loss: 0.6963\nEpoch 41/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.4795 - loss: 0.7313 - val_accuracy: 0.6452 - val_loss: 0.6789\nEpoch 42/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5790 - loss: 0.6828 - val_accuracy: 0.6774 - val_loss: 0.6542\nEpoch 43/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5575 - loss: 0.8002 - val_accuracy: 0.6452 - val_loss: 0.6670\nEpoch 44/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.5707 - loss: 0.7814 - val_accuracy: 0.6774 - val_loss: 0.6717\nEpoch 45/45\n\u001b[1m8/8\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.5826 - loss: 0.6935 - val_accuracy: 0.6452 - val_loss: 0.6755\n\n Test Accuracy: 0.6579\n\u001b[1m2/2\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    negative       0.67      0.70      0.68        20\n    positive       0.65      0.61      0.63        18\n\n    accuracy                           0.66        38\n   macro avg       0.66      0.66      0.66        38\nweighted avg       0.66      0.66      0.66        38\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 600x500 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":" Model saved successfully!\n","output_type":"stream"}],"execution_count":56},{"cell_type":"markdown","source":"# Attempt 1","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}